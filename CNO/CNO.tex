%% Credits of this ams template are with respective people. @Devansh1106 neither own this template nor the credits. 
\documentclass[reqno,10pt]{amsart}
\usepackage[a4paper, margin=1.25in]{geometry} % Change 1in to desired size
\usepackage[numbers]{natbib}
\setlength{\bibsep}{8pt}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{hyperref}
\usepackage{lineno}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{esint}
\usepackage{fancyhdr}
\setlength{\parskip}{0.5\baselineskip}%
\setlength{\parindent}{20pt}
\theoremstyle{plain}

\pagestyle{fancy}
\fancyhf{} % Clear default headers/footers
\setlength{\headheight}{10.0pt}
% Even pages: Paper title
\fancyhead[LE]{\footnotesize\textit{Universality for CNOs in setting of ANOs}}

% Odd pages: Author name
\fancyhead[RO]{\footnotesize\textit{Devansh Tripathi}}

\newtheorem*{thm*}{Theorem}
%% this allows for theorems which are not automatically numbered
\newcommand{\sinc}{\text{sinc}}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{rem}{Remark}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cal}[1]{\mathcal{#1}}

\hypersetup{
    colorlinks=false,
    linkcolor=blue,    % Internal links (sections, equations)
    citecolor=red,     % Citation links
    urlcolor=magenta   % URLs (DOIs, websites)
}
%% The above lines are for formatting.  In general, you will not want to change these.

\title{Report on the paper ``Convolutional Neural Operator for robust and accurate learning of PDEs''}
\author{Devansh Tripathi$^1$ \\ ETH Z\lowercase{\"urich}}
\thanks{$^1$Seminar für Angewandte Mathematik, HG E 62.2, Rämistrasse 101, 8092 Zürich, Switzerland \\ \href{mailto:devansh.tripathi@sam.math.ethz.ch}{\texttt{devansh.tripathi@sam.math.ethz.ch}}}

\begin{document}
\numberwithin{equation}{section}

\begin{abstract}
    In the paper \cite{BR2023}, the author argues that the convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ingnored in the context of learning solution operators of PDEs. The author present a novel framework termed as convolutional neural operators (CNOs) that is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. The author also proved a universal approximation result for CNOs.
\end{abstract}
\maketitle
\section{\bf \large Introduction}
    Given the ubiquitous nature of partial differential equations (PDEs) as mathematical models in the science and engineering, it becomes important to develop method to approximate the solutions to a PDE with less computational cost. There are well-established numerical methods such as finite differences, finite elements, finite volumes and spectral methods that have been successfully used to approximate PDE solution operator. Hwoever, the high computational cost of these methods, particularly in high dimensions and for {\it many query} problems such Uncertainity Quantification (UQ), inverse problems etc. calls upon the design of {\it fast, robus and accurate} surrogates.

    \noindent As {\it operators} are the objects of interest in solving PDEs, learning such operators from data which is loosely termed as {\it operator learning}, has emerged as a dominant paradigm in recent years. As it is argued in a recent paper \cite{FB2023}, a structure-preserving operator learning algorithm or {\it representation equivalent neural operator} has to respect some form of continuous-discrete equivalence (CDE) in order to learn the underlying operator, rather than just a discrete representation of it. Failure to respect such a CDE can lead to the so-called aliasing errors \cite{FB2023} and affect model performance at multiple discrete resolutions.

    \noindent The naive use of convolutional neural networks (CNNs) in the context of operator learning, see \cite{FB2023,Zhu2018,ZL2021} on how using CNNs for operator learning leads to results that heavily rely on the underlying grid resolution. The author has made the following contributions in this paper:
    \begin{itemize}
        \item The author proposes novel modifications to CNNs in order to enforce structure-preserving continuous-discrete equivalence (CDE) and enable the genuine, alias-free, learning of operators. The  resulting architecture, termed as {\it Convolutional Neural Operator}(CNO), is provided as novel {\it operator} adaptation of the widely used U-Net architecture.
        \item The author has shown that CNO is a {\it representation equivalent neural operator} in the sense of \cite{FB2023}, and also proved a universality result for CNOs to any desired accuracy.
        \item CNO has been tested on a {\it novel} set of benchmarks, known as {\it Representative PDE Benchmarks}(RPB), that span across a variety of PDEs ranging from linear elliptic and hyperbolic to nonlinear parabolic and hyperbolic PDEs, with possibly {\it multiscale solutions}. 
    \end{itemize}

    \section{Convolutional Neural Operator}
    \paragraph{\bf Setting} For simplicity, we will focus here on the two-dimensional case by specifying the underlying domain as $D = \bb T^2$, being the $2$-d torus. Let $\cal X = H^r(D,\bb R^{d_{\cal X}}) \subset \cal Z$ and $\cal Y = H^s(D,\bb R_{d_\cal Y})$ be the underlying function spaces, where $H^{r,s}(D,\cdot)$ are sobolev spaces of order $r$ and $s$. Without loss of generality, we set $r = s$ hereafter. Our aim would be to aproximate {\it continuous operators} $\cal G^\dag : \cal X \to \cal Y$ from data pairs $(u_i, \cal G^\dag(u_i))_{i=1}^M \in \cal X \times \cal Y$. We furthur assume that there exists a {\it modulus of continuity} for the operator i.e.,
    \begin{equation}\label{eq2.1}
        \|\cal G^\dag(u) - \cal G^\dag(v)\|_{\cal Y} \leq \omega(\|u - v\|_{\cal Z}), \qquad \forall u,v \in \cal X,
    \end{equation}
    with $\omega : \bb R_+ \to \bb R_+$ being a monotonically increasing function with $\lim_{y\to 0}\omega(y) = 0$ (implies that the operator $\cal G^\dag$ is uniformly continuous) The underlying operator $\cal G^\dag$ can corresponds to solution operators for PDEs but is more general that that and encompasses examples such as those arising in inverse problems, for instance in imaging.
    \paragraph{\bf Bandlimited Approximation} As argued in the paper \cite{FB2023} that Sobolev spaces such as $H^r$ are, in a sense, too large to allow for any {\it continuous-discrete equivalence} (CDE), i.e. equivalence between the underlying operator and its discrete representations, which is necessary for robust operator learning. We have to consider small subspaces of $H^r$ which allow for such CDEs. We choose the space of {\it bandlimited functions} defined by,
    \begin{equation}
        B_w(D) = \{f \in L^2(D) : \text{supp}\hat{f} \subseteq [-w,w]^2\},
    \end{equation}
    for some $w > 0$ and with $\hat{f}$ denoting the Fourier transform of $f$. It is shown in appendix \ref{appendix:A.1} that for any $\varepsilon > 0$, there exists a $w$, large enough depending on $r$, and a continuous operator $\cal G^* : B_w(D) \to B_w(D)$, such that $\|\cal G^\dag - \cal G^*\| \leq \varepsilon,$ with $\|\cdot\|$ denoting the corresponding operator norm. Along with that \ref{appendix:A.2} shows that we can define discrete versions of $\cal G^*$ using the underlying $sinc$ basis for bandlimited functions and establish a continuously-discrete equivalence for it.

    \paragraph{\bf Definition of CNO} Given above context, our goal will be approximate the operator $\cal G^*$ in a {\it structure-preserving manner} i.e. as the underlying operator maps between spaces of bandlimited functions, we will construct out operator approximation architecture to also map bandlimited functions to bandlimited functions, thus respecting the continuous-discrete equivalence.

    \noindent We denote the operator $\cal G : B_w(D) \to B_w(D)$ as a {\it convolutional neural operator} (CNO) which we define as a compositional mapping between functions as
    \begin{equation}\label{eq2.3}
        \cal G : u \mapsto P(u) = v_0 \mapsto v_1 \mapsto \dots v_L \mapsto Q(v_L) = \overline{u},
    \end{equation}
    where
    \begin{equation}
        v_{l+1} = \cal P_l \circ \Sigma_l \circ \cal K_l(v_l), \qquad 1 \leq l \leq L - 1.
    \end{equation}
    From \ref{eq2.3}, we see that first, the input function $u \in B_w(D)$ is lifted to the latent space of bandlimited functions through a {\it lifting layer}: 
    $$ P : \left\{u \in B_w(D,\bb R^{d_{\cal X}}) \right\} \to \left\{u \in B_w(D,\bb R^{d_0}) \right\}$$ 
    Here, $d_0 > d_{\cal X}$ is the number of channels in the lifted, latent space. The lifting operation is performed by a convolution operator which will be defined below.

    \noindent Then, the lifted function is processed through the composition of a series of mappings between functions (layers), with each layer consisting of three elementary mappings, i.e. $\cal P_l$ is either the {\it upsampling} or {\it downsampling} operator, $\cal K_l$ is the convolution operator and $\Sigma_l$ is the activation operator $Q$, defined as 
    $$ Q : \left\{v_L \in B_w(D,\bb R^{d_L}) \right\} \to \left\{\overline{u} \in B_w(D,\bb R^{d_{\cal Y}}) \right\}.$$
    The projection operation is also performed by a convolution operator defined below.

    \paragraph{\bf Convolution Operator} For simplicity, {\it single channel} version of the convolution operator $K_l$ is presented here. See appendix \ref{} for {\it multi-scale} version. Convolution operations are performed with discrete kernals
    $$ K_w = \sum_{i,j=1}^{k} k_{ij} \cdot \delta_{z_{ij}}$$
    defined on a $s \times s$ uniform grid on $D$ with grid size $\geq 1/2w$, in-order to satisfy the requirements of the Whittaker-Shannon Kotelnikov sampling theorem \cite{MU2000}, and $z_{ij}$ being the resulting grid points, $k \in\bb N$ being kernel size and $\delta_x$ denoting the Dirac measure at point $x\in D$. The convolution operator for a {\it single-channel} $K_w : B_w(D) \to B_w(D)$ is defined by 
    $$ \cal K_wf(x) = (K_w \star f)(x) = \int_D K_w(x-y)f(y) dy = \sum_{i,j=1}^{k} k_{i,j} f(x-z_{ij}), \qquad \forall x\in D,$$
    where the last identity arises from the fact that $f \in B_w$.















    \appendix
    \section{\bf Detailed proofs}
    \subsection[A.1]{Approximation of Operators mapping between Sobolev spaces by operators mapping between spaces of bandlimited functions} \label{appendix:A.1}
    We prove that one can approximate any continuous operator $\cal G^\dag : \cal X \to \cal Y$ by an operator mapping between spaces of bandlimited functions to arbitrary accuracy. We obtain this result by dsicarding the high-frequency components, higher than the frequency $w$, of both the input and the output of $\cal G^\dag$ by a Fourier projection $P_w$. For orthogonal Fourier projections and also trigonometric polynomial interpolation, the following result on the accuracy of the projection holds,

    \begin{lem*}[A.1] \label{lemA1}
        Given $\sigma, r \in \bb N_0$ with $r > d/2$ and $r \geq \sigma$, and $f \in C^r(\bb T^d)$ it holds for every $w \in \bb N$ that, 
        \begin{equation}
            \|f - P_w(f)\|_{H^\sigma(\bb T^d)} \leq C(r,d)w^{-(r-\sigma)}\|f\|_{H^r(\bb T^d)},
        \end{equation}
        for a constant $C(r,d) > 0$ that only depends on $r$ and $d$.
    \end{lem*}
    By choosing an appropriate frequency cutoff and then discarding the high frequencies of the input and output of $\cal G^\dag$ one can approximate $\cal G^\dag$ to arbitrary accuracy as shown in the result below.
    
    \begin{lem*}[A.2]
        For any $\epsilon, B > 0$ there exist $w \in \bb N$ such that $\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2(D)} \leq \epsilon$ for all $a \in H^r(D)$ with $\|a\|_{H^r(D)} \leq B$.        
    \end{lem*}
    \begin{proof}
        Using Lemma \ref{lemA1} and stability of $\cal G^\dag$, equation \ref{eq2.1}, we have
        $$
        \begin{aligned}
            \|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2} &\leq \|\cal G^\dag(a) - P_w\cal G^\dag(a)\|_{L^2} + \|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2}
        \end{aligned}
        $$
        Since, $\cal G^\dag : H^r(D;\bb R^{d_{\cal X}}) \to H^s(D;\bb R^{d_{\cal Y}})$, we have $\cal G(a) \in H^s(D;\bb R^{d_{\cal Y}})$. In order to apply Lemma \ref{lemA1}, we need to show that $\cal G^\dag(a) \in C^r(D)$. Note that $H^s(D) \hookrightarrow C^r(D)$ for all $s > r + d/2$ which will imply that $\cal G^\dag (a) \in C^r(D)$. Also, note that $P_w$(Fourier projection operator) is {\it non-expansive} ($L^2$-norm is less than $1$). Taking $\sigma = 0$ in Lemma \ref{lemA1}, we have 
        \begin{equation}            
        \begin{aligned}
            \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} &\leq \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{L^2} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \\
            (\because H^0(D) = L^2(D)) \qquad &= \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{H^0} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2}\\
            (\text{Lemma A.1 and non-expansive}) \qquad &\lesssim w^{-r} \left\|\cal G^\dag (a)\right\|_{H^r} + \left\|\cal G^\dag (a) - \cal G^\dag(P_wa)\right\|_{L^2}\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega(\|a - P_wa\|_{H^\sigma})\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega (Cw^{-(r-\sigma)}\|a\|_{H^r}).
        \end{aligned}
    \end{equation}
    \end{proof}
    Since above relation is true for all $a$ such that $\|a\|_{H^r} \leq B$, it follows that for large enough $w$:
    \begin{equation}
        \sup\limits_{\|a\|_{H^r}\leq B} \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \leq \epsilon.
    \end{equation}
    Given that both $P_wa \in B_w(D)$ and $P_w\cal G^\dag(P_wa) \leq B_w(D)$, a consequence of the above lemma is the existence of an operator $\cal G^* " B_w(D) \to B_w(D):a \mapsto P_w(\cal G^\dag(a))$ that can approximate $\cal G^\dag$ arbitrarily well. Hence $\left\|\cal G^\dag - \cal G^*\right\|_{op} \leq \epsilon$, where the operator are considered as mapping from and to $B_w(D) \cap H^r(D)$ equipped with the $H^r(D)$-norm.

    \subsection{Continuous-Discrete Equivalence for Operator \texorpdfstring{$\cal G^*$}{}} \label{appendix:A.2}
    For every $w > 0$, we denote by $B_w(\bb R^2)$ the space of multivariate bandlimited functions
    $$B_w(\bb R^2) = \{f \in L^2(\bb R^2) : \text{supp}\hat{f} \subset eq [-w,w]^2\},$$
    where $\hat{f}$ denotes the Fourier transform on $L^1(\bb R)$ 
    $$ \hat{f}(\xi) := \int_{\bb R}f(x) e^{-2\pi ix\xi} dx, \qquad \xi \in \bb R,$$
    which extends to $L^2(\bb R)$ by a classical density argument ($L^1(\bb R^n) \cap L^2(\bb R^n)$ is dense in $L^2(\bb R^n), n \geq 1$.) The set $\Psi_w = \{\sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n)\}_{m,n \in \bb Z}$ constitutes an orthonormal basis for $B_w(\bb R^2)$. The bounded operator
    $$ T_{\Psi_w} :l^2(\bb Z^2) \to B_w(\bb R^2), ~~ T_{\Psi_w}(c_{m,n}) = \sum_{m,n\in \bb Z} c_{m,n} \sinc(2w\cdot - m)\cdot \sinc(2w\cdot -n),$$
    which reconstructs a function from its basis coefficients, is called {\it synthesis operator,} and its adjoint 
    $$ T^*_{\Psi_w} : B_w(\bb R^2) \to l^2(\bb Z^2), \qquad T^*_{\Psi_w} f = \left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}_{m,n \in \bb Z},$$
    which extracts basis coefficients from an underlying function, is called {\it analysis operator}.  Every bandlimited function can be uniquely and stably recovered from its sampled values $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$ for ${m,n \in \bb Z}$ via the reconstruction formula
    \begin{equation}
        f(x_1,x_2) = T_{\Psi_w}T^*_{\Psi_w}f(x_1,x_2) = \sum\limits_{m,n \in \bb Z} f\left(\frac{m}{2w},\frac{n}{2w}\right) \sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n),
    \end{equation}
    and we say that there is a {\it continuous-discrete equivalence (CDE)} between $f$ and its samples $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$. In general, every bandlimited function $f \in B_w(\bb R^2)$ cam be uniquely and stably recovered from its sample values $\{f(mT,nT)\}_{m,n\in\bb Z}$ if the {\it sampling rate} or reciprocal of grid size, $1/T$ is greater or equal than the {\it Nyquist rate} $2w$. This simply follows from the fact that $B_w(\bb R^2) \subset B_{w'}(\bb R^2)$ for every $w' > w$. On the contrary, reconstructing $f \in B_w$ at a sampling rate below the Nyquist rate, i.e. $1/T < 2w$, results in a non-zero value for the {\it aliasing error function:}
    $$ \epsilon(f) = f - T_{{\Psi}_{\frac{1}{2T}}} T^*_{{\Psi}_{\frac{1}{2T}}}f,$$
    and the associated {\it aliasing error} $\|\epsilon\|_2$  (see defintion below).
    \begin{defn}[Aliasing for bandlimited functions]
        The aliasing error function $\varepsilon(f)$ and the corresponding aliasing error of $f \in L^2(\bb R)$ for sampling at the rate $2\Omega$ are given by
        $$ \varepsilon(f) = f - \cal P_{B_\Omega}f, \qquad\qquad \|\varepsilon(f)\|_2 = \|f - \cal P_{B_\Omega}f\|_2.$$
        where $\cal P_{B_\Omega} : L^2(\bb R) \to B_\Omega$ is the {\it orthogonal projection operator} onto $B_\Omega$ given by $P_{B_\Omega}f = \sum_{n\in \bb Z}\langle f,\phi_n\rangle\phi_n$. If the aliasing error $\varepsilon(f)$ is zero, i.e. if $f \in B_\Omega$, we say that there is a continuous-discrete equivalence (CDE) between $f$ and its samples $\{f(n/2\Omega)\}_{n \in \bb Z}$.
    \end{defn}
    \noindent Let $\cal G^*$ be a (possibly) non-linear operator between band-limited spaces, i.e. $\cal G^* :B_w(\bb R^2) \to B_{w'}(\bb R^2)$, for some $w,w' > 0$. As given in \ref{appendix:C}, the concepts of continuous-discrete equivalence (CDE) and aliasing error can be adapted to the operator $\cal G^*$. The continuous operator $\cal G^*$ is uniquely determined by a map $\mathfrak{g}_{\Psi_w, \Psi_{w'}}:\ell^2(\bb Z^2) \to \ell^2(\bb Z^2)$ if the aliasing error operator.
    \begin{equation}\label{eq:opaliaserror}
        \varepsilon = \cal G^* - T_{\Psi_{w'}} \circ \mathfrak{g}_{\Psi_w, \Psi_{w'}} \circ T^*_{\Psi_w}
    \end{equation}
    is identically zero, and we say that $\cal G^*$ and $\mathfrak{g}_{\Psi_w, \Psi_{w'}}$, satisfy a continuous-discrete equivalence (Definition \ref{def:opalias}). Equivalently, the diagram 
    \begin{figure}[!ht]
        \begin{tikzcd}
            B_w \arrow[r, "\cal G^*"] \arrow[d, blue, "T^*_{\Psi_w}"] & B_{w'}, \\
            \ell^2(\bb Z^2) \arrow[r, blue, "\mathfrak{g}_{\Psi_w, \Psi_{w'}}"] & \ell^2(\bb Z^2) \arrow[u, blue, "T_{\Psi_{w'}}"]
        \end{tikzcd}
    \end{figure}
    commutes, i.e. the black and the blue directed paths in the diagram lead to the same result. In the latter case, since $T^*_{\Psi_w} \circ T_{\Psi_w}$ is identity operator from $\ell^2(\bb Z^2)$ onto itself ($\because \Psi_w$ is taken to be orthonormal basis set, otherwise it is Gram matrix $G_{mn} := \langle \Psi_n,\Psi_m\rangle$), equation \ref{eq:opaliaserror} forces the discretization $\mathfrak{g}_{\Psi_w,\Psi_{w'}},$ to be defined as
    \begin{equation}\label{eq:defg}
        \mathfrak{g}_{\Psi_w,\Psi_{w'}} := T^*_{\Psi_{w'}} \circ \cal G^* \circ T_{\Psi_w}.
    \end{equation}
    The above defintion is motivated from the fact that $\Psi_w$ is orthonormal set and the equation \ref{eq:opaliaserror} when $\varepsilon = 0$.
    \begin{rem}\label{rem:invert}
        Note that in general $T_{\Psi_w}$ is not invertible. When the frame sequence is orthonormal, it is invertible and its inverse is same as its adjoint.
    \end{rem}
    \begin{rem}
        While isolating $\mathfrak{g}_{\Psi_w,\Psi_{w'}}$ on LHS in equation \ref{eq:opaliaserror}, the term $T^{-1}_{\Psi_{w'}}$ will appear and it has been replaced to $T^*_{\Psi_{w'}}$ in equation \ref{eq:defg} because of remark \ref{rem:invert}. Why? That is how we define it. 
    \end{rem}
    \noindent The definition \ref{eq:defg} is equivalent to say that the diagram,
    \begin{figure}[!ht]
        \begin{tikzcd}
            B_w \arrow[r, blue, "\cal G^*"] & B_{w'} \arrow[d, blue, "T^*_{\Psi_{w'}}"], \\
            \ell^2(\bb Z^2) \arrow[r, "\mathfrak{g}_{\Psi_w, \Psi_{w'}}"] \arrow[u, blue, "T_{\Psi_w}"] & \ell^2(\bb Z^2) 
        \end{tikzcd}
    \end{figure}
    commutes. In other words, once we fix the discrete representation associcated to the input and output functions, there exists a unique way to defined a discretization $\mathfrak{g}_{\Psi_w,\Psi_{w'}}$ that is consistent with the continuous operator $\cal G^*$ and this is given by \ref{eq:defg}.

    \noindent In practice, we may have access to different discrete representations of the input and output functions, e.g. point samples evaluated on different grids, which in the theory amounts to change of reference systems in the functions spaces. For instance, sampling a function $f\in B_w$ on a finer gris $\left\{(\frac{m}{2\overline{w}},\frac{n}{2\overline{w}})\right\}_{m,n \in \bb Z},\overline{w} > w,$ amounts to representing the function $f$ with respect to the system $\Psi_{\overline{w}} = \{\sinc(2\overline{w}x_1 - m) \cdot \sinc(2\overline{w}x_2 - n)\}_{m,n \in \bb Z},$ which constitutes an orthonormal basis for $B_{\overline{w}} \supset B_w$. Then, one can define the associated CDE discretization $\mathfrak{g}_{\Psi_{\overline{w}},\Psi_{\overline{w}'}}$ as in \ref{eq:defg}, and by equation \ref{eq:opaliaserror}, one readily obtains the change of basis formula
    \begin{equation}
        \mathfrak{g}_{\Psi_{\overline{w}},\Psi_{\overline{w}'}} = T^*_{\Psi_{\overline{w}'}} \circ T_{\Psi_{w'}} \circ \mathfrak{g}_{\Psi_w,\Psi_{w'}} \circ T^*_{\Psi_w} \circ T_{\Psi_{\overline{w}}},
    \end{equation}
    Finally, all the above concepts generalize to every pair of frame sequences $(\Psi,\Psi)$ that span respectively the input and output function spaces, and we refer \cite{FB2023} for a complete exposition.
    \section{\bf An Introduction to Frame Theory} \label{appendix:B}
    \noindent Let $\cal H$ be a separable Hilbert space with inner product $\langle\cdot,\cdot\rangle$ and norm $\|\cdot\|$.
    \begin{defn}[Frame]
        A countable sequence of vectors $\{f_i\}_{i\in I}$ in $\cal H$ is a {\it frame} for $\cal H$ if there exists constants $A,B > 0$ such that for all $f \in \cal H$
        $$ A\|f\|^2 \leq \sum_{i\in I} |\langle f,f_i\rangle|^2 \leq B\|f\|^2.$$
        We say that $\{f_i\}_{i\in I}$ is a {\it tight frame} if $A = B$ and, in particular, a {\it Parseval fram} if $A = B = 1$. 
    \end{defn}
    \begin{rem}
        The condition in the above definition ensure that $f$ can be stably reconstructed from its inner products with the frame elements. Stable reconstruction here implies the well-posedness and bounded in both direction (no explosion or vanishing of norm).
    \end{rem}
    \begin{rem}
        A frame is complete in $\cal H$, stable spanning set of $\cal H$.
    \end{rem}
    Clearly by the Parseval identity, an orthonormal basis for $\cal H$ is a Parseval frame ($\because \|f\|^2 = \sum_{n=1}^{\infty}|\langle x,x_n\rangle|^2$, by Parseval's identity for a separable Hilbert space, the so called generalization of the Pythagorean theorem). The lower inequality implies that 
    $$ \langle f,f_i\rangle = 0, \forall i \in I \implies f = 0,$$
    which is equivalent to 
    $$ \overline{\text{span}}\{f_i : i \in I\} = \cal H.$$
    On the other hand the upper inequality implies that the operator
    $$ T:l^2(I) \to \cal H, \qquad T(\{c_i\}_{i\in I}) = \sum_{i\in I} c_if_i,$$
    is bounded with $\|T\| \leq \sqrt{B}$ (\cite{Christensen2008-gk} Theorem 3.1.3), and we call $T$ the {\it synthesis operator}. Its adjoint is given by 
    $$ T^* : \cal H \to \cal H, \qquad Sf = TT^*f = \sum_{i\in I} \langle f,f_i\rangle f_i,$$
    which is bounded, invertible, self-adjoint and positive operator (\cite{Christensen2008-gk}, Lemma 5.1.6). We note that the frame operator is invertible since it is bounded, being a composition of two bounded operator, and the frame property implies that $\|\text{Id} - B^{-1}S\| < 1$, where Id denotes the identity operator. Furthermore, the pseudo-inverse of the synthesis operator is given by
    $$ T^\dag : \cal H \to l^2(I), \qquad T^\dag f = (\langle f, S^{-1}f_i\rangle)_{i\in I},$$
    (\cite{Christensen2008-gk}, Theorem 5.3.7) and $\|T^\dag\| \leq 1/\sqrt{A}$(\cite{Christensen2008-gk}, Proposition 5.3.8). The composition $TT^\dag$ gives the identity operator on $\cal H$, and consequently every element in $\cal H$ can be reconstructed via the reconstruction formula
    \begin{equation}\label{formula}
        f = TT^\dag f= \sum_{i\in f}\langle f,S^{-1}f_i\rangle f_i = \sum_{i\in I}\langle f,f_i\rangle S^{-1}f_i,
    \end{equation}
    where the series converge unconditionally. Formula \ref{formula} is known as the {\it frame decomposition thoerem} (\cite{Christensen2008-gk}, Theorem 5.1.7). In particular, if $\{f_i\}_{i\in I}$ is a tight frame, then $S = A\text{Id}$ and formula \ref{formula} simply reads
    $$ f = \frac{1}{A}\sum_{i\in I}\langle f,f_i\rangle f_i.$$
    On the other hand, the composition $T\dag T$ gives the orthogonal projection of $l^2(I)$ onto Ran($T^\dag$)(\cite{Christensen2008-gk}, Lemma 2.5.2).

    \noindent In what follows, we consider sequences which are not complete in $\cal H$, and consequently are not frames for $\cal H$, but they are frames for their closed linear span.
    \begin{defn}[Frame sequence]
        Let $\{v_i\}_{i\in I}$ be a countable sequence of vectors in $\cal H$. We say that $\{v_i\}_{i\in I}$ is a frame sequence {\it if it is a frame for $\overline{span}\{v_i:i\in I\}$.}
    \end{defn}
    \begin{rem}
        A frame sequence may not span all of $\cal H$, but it is a frame for the subspace of $\cal H$ which it does span.
    \end{rem}
    \noindent A frame sequence $\{v_i\}_{i\in I}$ in $\cal H$ with synthesis operator $T : l^2(I) \to \overline{span}\{v_i : i\in I\}$ is a frame for $\cal H$ if and only if $T^*$ is injective, whilst in general $T^*$ is not surjective and consequently $T$ is not injective. We denote $\cal V = \overline{span}\{v_i : i\in I\}$. Then, the orthogonal projection of $\cal H$ onto $\cal V$ is given by
    $$ \cal P_{\cal V}f = TT^\dag = \sum_{i\in I}\langle f,S^{-1}v_i\rangle v_i,$$
    where $S : \cal V \to \cal V$ denotes the frame operator. Hence, reconstruction formula \ref{formula} holds if and only if $f\in \cal V$.

    \section{\bf Alias-Free Framework for Operator Learning} \label{appendix:C}
    \noindent In this section, we will extend this concept of aliasing of function to operators. Let $U$ be a operator mapping between infinite-dimensional function spaces and $u$ be a discrete representation mapping.
    
    \paragraph{\bf Setting}Let $U: \text{Dom }U \subseteq \cal H \to \cal K$ be an operator between two separable Hilbert spaces, and let $\Psi = \{\psi_i\}_{i\in I}$ and $\Psi = \{\phi_k\}_{k \in K}$ be frame sequences for $\cal H$ and $\cal K$, respectively, with synthesis operators $T_\Psi$ and $T_\Phi$. We denote their closed linear spans by $\cal M_\Psi := \overline{\text{span}}\{\psi_i : i \in I\}$ and $\cal M_\Phi := \overline{\text{span}}\{\phi_i : k \in K\}$. We note that by classical frame theory \cite{Christensen2008-gk}, the pseudo-inverses $T_\Psi^\dag$ and $T_\Phi^\dag$, initially defined on $\cal M^\Psi$ and $\cal M_\Psi$, respectively, can in fact be extended to the entire Hilbert spaces, i.e. $T_\Psi^\dag : \cal H \to l^2(I)$ and $T_\Phi^\dag : \cal K \to l^2(K)$ and this extension is given explicitly via the dual frame analysis operator.
    \subsection{\bf Operator Aliasing and Representation Equivalence}
    Once the discretization is chosen -- which is equivalent to choosing the frame sequences for input and output spaces $(\Psi,\Phi)$ -- connecting the continuous operator $U$ with its discrete counterpart $u$ is the notion of operator aliasing. Given any mapping $u : l^2(I) \to l^2(K)$, we can build the operator $T_\Phi \circ u \circ T_\Psi^\dag : \cal H \to \cal K$, whose definition clearly on the choices of the frame sequences that we make on the continuous level. In other words, any mapping $u$ can be interpreted as a discrete representation of an underlying continuous operator, which in general, may differ from the operator $U$, that is of interest here. Hence, in analogy to definition of aliasing for functions in Hilbert space, we can define the aliasing error of $U$ relative to the discrete representation $u$ as,
    \begin{defn}[Operator aliasing]\label{def:opalias}
        The aliasing error operator $\varepsilon(U,u,\Psi,\Phi):\text{Dom }U \subseteq \cal H \to \cal K$ is given by
        $$ \varepsilon(U,u,\Psi,\Phi) = U - T_\Phi \circ u \circ T_\Psi^\dag,$$
        and the corresponding scalar error is $\|\varepsilon(U, u, \Psi, \Phi)\|$, with $\|\cdot\|$ denoting the operator norm.
    \end{defn}
    \noindent An aliasing error of zero implies that the operator $U$ can be perfectly represented by first discretizing the function with $T_\Psi^\dag$, applying $u$, and then reconstructing with $T_\Phi$, or equivalently, that the diagram in Figure \ref{fig:1} commutes, i.e. black and blue directed paths in the diagram lead to the same result. If the aliasing error is zero, we say that $(U, u, \Psi, \Phi)$ satisfies a {\it continuous-discrete equivalence (CDE)}, implying that accessing the discrete representation $u$ is exactly same as accessing the underlying continuous operator $U$.
    \begin{figure}[!ht] \label{fig:1}
        \begin{tikzcd}
        \cal H \arrow[r, "U"] \arrow[d, blue, "T_\Psi^\dag"] & \cal K \\
        \ell^2(I) \arrow[r, blue, "u"] & \ell^2(K) \arrow[u, blue, "T_\Phi"]
        \end{tikzcd}
        \caption{An alias-free operator's diagram commutes.}
    \end{figure}
    In practice, the discrete representation $u$ of the operator $U$ depends on the choice of the frame sequences. This means that -- as mentioned at the beginning of the section -- there is one map for every input/output frame sequence $\Phi,\Psi$. The consistency between operations $u,u'$ with respect to different frame sequences can be evaluated using the following error.
    \begin{defn}[Representation equivalence error] Suppose that $u,u'$ are discrete maps with associated frame sequences $\Psi,\Phi$ and $\Psi',\Phi'$, respectively. Then, the representation equivalence error is given by the function $\tau(u,u') : \ell^2(I) \to \ell^2(K),$ defined as:
    $$ \tau(u,u') = u - T^\dag_\Phi \circ T_{\Phi'} \circ u' \circ T_{\Psi'}^\dag \circ T_\Psi$$
    and the corresponding scalar error is $\|\tau(u,u')\|$.
    \end{defn}
    \noindent Intuitively, this amount to computing each mapping on their given discretization, and comparing them by expressing $u'$ in the frames associated to $u$

\bibliographystyle{plainnat}
\bibliography{ref_cno}
\end{document}