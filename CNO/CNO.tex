%% Credits of this ams template are with respective people. @Devansh1106 neither own this template nor the credits. 
\documentclass[reqno,10pt]{amsart}
\usepackage[a4paper, margin=1.25in]{geometry} % Change 1in to desired size
\usepackage[numbers]{natbib}
\setlength{\bibsep}{8pt}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{subcaption}
\captionsetup[subfigure]{labelformat=parens,labelfont=normalfont}
\renewcommand{\thesubfigure}{\alph{subfigure}}
\usepackage{hyperref}
\usepackage{lineno}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{esint}
\usepackage{fancyhdr}
\setlength{\parskip}{0.5\baselineskip}%
\setlength{\parindent}{20pt}
\theoremstyle{plain}

\pagestyle{fancy}
\fancyhf{} % Clear default headers/footers
\setlength{\headheight}{10.0pt}
% Even pages: Paper title
\fancyhead[LE]{\footnotesize\textit{Universality for CNOs in setting of ANOs}}

% Odd pages: Author name
\fancyhead[RO]{\footnotesize\textit{Devansh Tripathi}}

\newtheorem*{thm*}{Theorem}
%% this allows for theorems which are not automatically numbered
\newcommand{\sinc}{\text{sinc}}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{rem}{Remark}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cal}[1]{\mathcal{#1}}

\hypersetup{
    colorlinks=false,
    linkcolor=blue,    % Internal links (sections, equations)
    citecolor=red,     % Citation links
    urlcolor=magenta   % URLs (DOIs, websites)
}
%% The above lines are for formatting.  In general, you will not want to change these.

\title{Report on the paper ``Convolutional Neural Operator for robust and accurate learning of PDEs''}
\author{Devansh Tripathi$^1$ \\ ETH Z\lowercase{\"urich}}
\thanks{$^1$Seminar für Angewandte Mathematik, HG E 62.2, Rämistrasse 101, 8092 Zürich, Switzerland \\ \href{mailto:devansh.tripathi@sam.math.ethz.ch}{\texttt{devansh.tripathi@sam.math.ethz.ch}}}

\begin{document}
\numberwithin{equation}{section}

\begin{abstract}
    In the paper \cite{BR2023}, the author argues that the convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ingnored in the context of learning solution operators of PDEs. The author present a novel framework termed as convolutional neural operators (CNOs) that is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. The author also proved a universal approximation result for CNOs.
\end{abstract}
\maketitle
\section{\bf \large Introduction}
    Given the ubiquitous nature of partial differential equations (PDEs) as mathematical models in the science and engineering, it becomes important to develop method to approximate the solutions to a PDE with less computational cost. There are well-established numerical methods such as finite differences, finite elements, finite volumes and spectral methods that have been successfully used to approximate PDE solution operator. Hwoever, the high computational cost of these methods, particularly in high dimensions and for {\it many query} problems such Uncertainity Quantification (UQ), inverse problems etc. calls upon the design of {\it fast, robus and accurate} surrogates.

    \noindent As {\it operators} are the objects of interest in solving PDEs, learning such operators from data which is loosely termed as {\it operator learning}, has emerged as a dominant paradigm in recent years. As it is argued in a recent paper \cite{FB2023}, a structure-preserving operator learning algorithm or {\it representation equivalent neural operator} has to respect some form of continuous-discrete equivalence (CDE) in order to learn the underlying operator, rather than just a discrete representation of it. Failure to respect such a CDE can lead to the so-called aliasing errors \cite{FB2023} and affect model performance at multiple discrete resolutions.

    \noindent The naive use of convolutional neural networks (CNNs) in the context of operator learning, see \cite{FB2023,Zhu2018,ZL2021} on how using CNNs for operator learning leads to results that heavily rely on the underlying grid resolution. The author has made the following contributions in this paper:
    \begin{itemize}
        \item The author proposes novel modifications to CNNs in order to enforce structure-preserving continuous-discrete equivalence (CDE) and enable the genuine, alias-free, learning of operators. The  resulting architecture, termed as {\it Convolutional Neural Operator}(CNO), is provided as novel {\it operator} adaptation of the widely used U-Net architecture.
        \item The author has shown that CNO is a {\it representation equivalent neural operator} in the sense of \cite{FB2023}, and also proved a universality result for CNOs to any desired accuracy.
        \item CNO has been tested on a {\it novel} set of benchmarks, known as {\it Representative PDE Benchmarks}(RPB), that span across a variety of PDEs ranging from linear elliptic and hyperbolic to nonlinear parabolic and hyperbolic PDEs, with possibly {\it multiscale solutions}. 
    \end{itemize}

    \section{Convolutional Neural Operator}
    \paragraph{\bf Setting} For simplicity, we will focus here on the two-dimensional case by specifying the underlying domain as $D = \bb T^2$, being the $2$-d torus. Let $\cal X = H^r(D,\bb R^{d_{\cal X}}) \subset \cal Z$ and $\cal Y = H^s(D,\bb R_{d_\cal Y})$ be the underlying function spaces, where $H^{r,s}(D,\cdot)$ are sobolev spaces of order $r$ and $s$. Without loss of generality, we set $r = s$ hereafter. Our aim would be to aproximate {\it continuous operators} $\cal G^\dag : \cal X \to \cal Y$ from data pairs $(u_i, \cal G^\dag(u_i))_{i=1}^M \in \cal X \times \cal Y$. We furthur assume that there exists a {\it modulus of continuity} for the operator i.e.,
    \begin{equation}\label{eq2.1}
        \|\cal G^\dag(u) - \cal G^\dag(v)\|_{\cal Y} \leq \omega(\|u - v\|_{\cal Z}), \qquad \forall u,v \in \cal X,
    \end{equation}
    with $\omega : \bb R_+ \to \bb R_+$ being a monotonically increasing function with $\lim_{y\to 0}\omega(y) = 0$ (implies that the operator $\cal G^\dag$ is uniformly continuous) The underlying operator $\cal G^\dag$ can corresponds to solution operators for PDEs but is more general that that and encompasses examples such as those arising in inverse problems, for instance in imaging.
    \paragraph{\bf Bandlimited Approximation} As argued in the paper \cite{FB2023} that Sobolev spaces such as $H^r$ are, in a sense, too large to allow for any {\it continuous-discrete equivalence} (CDE), i.e. equivalence between the underlying operator and its discrete representations, which is necessary for robust operator learning. We have to consider small subspaces of $H^r$ which allow for such CDEs. We choose the space of {\it bandlimited functions} defined by,
    \begin{equation}
        B_w(D) = \{f \in L^2(D) : \text{supp}\hat{f} \subseteq [-w,w]^2\},
    \end{equation}
    for some $w > 0$ and with $\hat{f}$ denoting the Fourier transform of $f$. It is shown in appendix \ref{appendix:A.1} that for any $\varepsilon > 0$, there exists a $w$, large enough depending on $r$, and a continuous operator $\cal G^* : B_w(D) \to B_w(D)$, such that $\|\cal G^\dag - \cal G^*\| \leq \varepsilon,$ with $\|\cdot\|$ denoting the corresponding operator norm. Along with that \ref{appendix:A.2} shows that we can define discrete versions of $\cal G^*$ using the underlying $sinc$ basis for bandlimited functions and establish a continuously-discrete equivalence for it.

    \paragraph{\bf Definition of CNO} Given above context, our goal will be approximate the operator $\cal G^*$ in a {\it structure-preserving manner} i.e. as the underlying operator maps between spaces of bandlimited functions, we will construct out operator approximation architecture to also map bandlimited functions to bandlimited functions, thus respecting the continuous-discrete equivalence.

    \noindent We denote the operator $\cal G : B_w(D) \to B_w(D)$ as a {\it convolutional neural operator} (CNO) which we define as a compositional mapping between functions as
    \begin{equation}\label{eq2.3}
        \cal G : u \mapsto P(u) = v_0 \mapsto v_1 \mapsto \dots v_L \mapsto Q(v_L) = \overline{u},
    \end{equation}
    where
    \begin{equation}
        v_{l+1} = \cal P_l \circ \Sigma_l \circ \cal K_l(v_l), \qquad 1 \leq l \leq L - 1.
    \end{equation}
    From \ref{eq2.3}, we see that first, the input function $u \in B_w(D)$ is lifted to the latent space of bandlimited functions through a {\it lifting layer}: 
    $$ P : \left\{u \in B_w(D,\bb R^{d_{\cal X}}) \right\} \to \left\{u \in B_w(D,\bb R^{d_0}) \right\}$$ 
    Here, $d_0 > d_{\cal X}$ is the number of channels in the lifted, latent space. The lifting operation is performed by a convolution operator which will be defined below.

    \noindent Then, the lifted function is processed through the composition of a series of mappings between functions (layers), with each layer consisting of three elementary mappings, i.e. $\cal P_l$ is either the {\it upsampling} or {\it downsampling} operator, $\cal K_l$ is the convolution operator and $\Sigma_l$ is the activation operator $Q$, defined as 
    $$ Q : \left\{v_L \in B_w(D,\bb R^{d_L}) \right\} \to \left\{\overline{u} \in B_w(D,\bb R^{d_{\cal Y}}) \right\}.$$
    The projection operation is also performed by a convolution operator defined below.

    \paragraph{\bf Convolution Operator} For simplicity, {\it single channel} version of the convolution operator $K_l$ is presented here. See appendix \ref{} for {\it multi-scale} version. Convolution operations are performed with discrete kernals
    $$ K_w = \sum_{i,j=1}^{k} k_{ij} \cdot \delta_{z_{ij}}$$
    defined on a $s \times s$ uniform grid on $D$ with grid size $\geq 1/2w$, in-order to satisfy the requirements of the Whittaker-Shannon Kotelnikov sampling theorem \cite{MU2000}, and $z_{ij}$ being the resulting grid points, $k \in\bb N$ being kernel size and $\delta_x$ denoting the Dirac measure at point $x\in D$. The convolution operator for a {\it single-channel} $K_w : B_w(D) \to B_w(D)$ is defined by 
    $$ \cal K_wf(x) = (K_w \star f)(x) = \int_D K_w(x-y)f(y) dy = \sum_{i,j=1}^{k} k_{i,j} f(x-z_{ij}), \qquad \forall x\in D,$$
    where the last identity arises from the fact that $f \in B_w$. Thus, our convolution operator is directly parameterized in physical space, in contrast to the Fourier space parameterization of convolution in the FNO architecture of \cite{ZL2021}. Hence, our parameterization is of a {\it local} nature.

    \paragraph{\bf Upsampling and Downsampling Operator} For some $\overline{w} > w$, we can {\it upsample} a function $f \in B_w$ to the {\it higher band} $B_{\overline{w}}$ by simple setting,
    $$ \cal U_{w,\overline{w}}: B_w(D) \to B_{\overline{w}}(D), \qquad \cal U_{w,\overline{w}} f(x) = f(x), \qquad \forall x\in D.$$
    On the other hand, for some $\underline{w} < w$, we can {\it downsample} a function $ f\in B_w$ to the {\it lower band} $B_{\underline{w}}$ by setting $\cal D_{w,\underline{w}} : B_w(D) \to B_{\underline{w}}(D),$ defined by 
    $$\cal D_{w,\underline{w}}f(x) = \left(\frac{\underline{w}}{w}\right)^2 (h_{\underline{w}} \star f)(x) = \left(\frac{\underline{w}}{w}\right)^2 \int_D h_{\underline{w}}(x-y) f(y) dy, \qquad \forall x\in D,$$
    where $\star$ is the convolution operation on functions defined above and $h_{\underline{w}}$ is the so-called {\it interpolation sinc filter}:
    \begin{equation}
        h_w(x_0,x_1) = \sinc(2wx_0) \cdot \sinc(2wx_1), \qquad (x_0,x_1) \in \bb R^2.
    \end{equation}

    \paragraph{\bf Activation Layer} Naturally, one can apply the activation function pointwise to any function. However, it is well-known that such an application will no longer respect the band-limits of the underlying function space and generate {\it aliasing error} \cite{FB2023}. In particular, nonlinear activations can generate features at arbitrarily high frequencies. Our aim is to respect the underlying CDE, we will modulate the application of the activation function so that the resulting outputs falls within desired band limits.

    \noindent First, we will upsample the input function $f \in B_w$ to a higher bandlimit $\overline{w} > w$, then apply the activation and finally downsample the result back to the original bandlimit $w$. Implicitly assuming that $\overline{w}$ is large enough such that $\sigma(B_w) \subset B_{\overline{w}}$, we define the activation layer as,
    \begin{equation}
        \Sigma_{w,\overline{w}} : B_w(D) \to B_w(D), \qquad \Sigma_{w,\overline{w}} f(x) = \cal D_{\overline{w},w}(\sigma \circ \cal U_{w,\tilde{w}} f)(x), \qquad \forall x\in D.
    \end{equation}

    \paragraph{\bf Instantiation through an Operator U-Net architecture} The above blocks are assembled together in the form of an Operator U-Net architecture that has bandlimited functions as input and outputs. We also need other ingredients such as {\it skip connections} through {\it ResNet} blocks of the form, $\cal R_{w,\overline{w}} : B_w(D,\bb R^d) \to B_w(D,\bb R^d),$ such that
    \begin{equation}
        \cal R_{w,\overline{w}}(v) = v + \cal K_w \circ \Sigma_{w,\overline{w}} \circ \cal K_w(v), \qquad \forall v \in B_w(D,\bb R^d).        
    \end{equation}
    we also need the {\it Invariant blocks} of the form , $\cal I_{w,\overline{w}}:B_w(D,\bb R^d) \to B_w(D,\bb R^d)$ such that 
    \begin{equation}
        \cal I_{w,\overline{w}}(v) = \Sigma_{w,\overline{w}} \circ \cal K_w(v), \qquad \forall v \in B_w(D,\bb R^d).
    \end{equation}

    \paragraph{\bf Continuous-Discrete Equivalence for CNO} We have defined CNO as an operator that maps bandlimited functions to bandlimited functions. In practice, CNO has to be implemented in a discrete manner with {\it discretized versions} of each of the above-defined elementary operations. We prove the following proposition:
    \begin{prop}
        Convolutional Neural Operator $\cal G: B_w(D,\bb R^{d_{\cal X}}) \to B_w(D,\bb R^{d_{\cal Y}})$ is a Representation equivalent neural operator or ReNO, in the sense of Defintion \ref{def:ReNO}.      
    \end{prop}
    \begin{proof}
        TODO
    \end{proof}

    \section{Universal Approximation by CNOs}
    \noindent We consider the following abstract PDE in the domain $D = \bb T^2$,
    \begin{equation}\label{eq:pde}
        \cal L(u) = 0, \qquad \cal B(u) = 0,
    \end{equation}
    with $\cal L$ being the differential operator and $\cal B$ a boundary operator. We assume that the differential operator $\cal L$ only depends on the coordinate $x$ through a {\it coefficient} function $a \in H^r(D).$ The corresponding {\it solution} operator is denoted by $\cal G^\dag : \cal X^* \subset H^r(D) \to H^r(D) : a \mapsto u,$ with $u$ beign the solution of the PDE (\ref{eq:pde}). We assume that $\cal G^\dag$ is continuous. Moreover, we also assume the following modulus of continuity,
    \begin{equation}
        \left\|\cal G^\dag(a) - \cal G^\dag(a')\right\|_{L^p(\bb T^2)} \leq \omega\left(\|a - a'\|_{H^\sigma(\bb T^2)}\right),
    \end{equation}
    for some $p \in \{2,\infty\}$ and $0 \leq \sigma \leq r-1$, and where $\omega : [0\infty) \to [0,\infty)$ is a monotonously increasing function with $\lim_{y \to 0} \omega (y) = 0$.
















    \appendix
    \section{\bf Detailed proofs}
    \subsection[A.1]{Approximation of Operators mapping between Sobolev spaces by operators mapping between spaces of bandlimited functions} \label{appendix:A.1}
    We prove that one can approximate any continuous operator $\cal G^\dag : \cal X \to \cal Y$ by an operator mapping between spaces of bandlimited functions to arbitrary accuracy. We obtain this result by dsicarding the high-frequency components, higher than the frequency $w$, of both the input and the output of $\cal G^\dag$ by a Fourier projection $P_w$. For orthogonal Fourier projections and also trigonometric polynomial interpolation, the following result on the accuracy of the projection holds,

    \begin{lem*}[A.1] \label{lemA1}
        Given $\sigma, r \in \bb N_0$ with $r > d/2$ and $r \geq \sigma$, and $f \in C^r(\bb T^d)$ it holds for every $w \in \bb N$ that, 
        \begin{equation}
            \|f - P_w(f)\|_{H^\sigma(\bb T^d)} \leq C(r,d)w^{-(r-\sigma)}\|f\|_{H^r(\bb T^d)},
        \end{equation}
        for a constant $C(r,d) > 0$ that only depends on $r$ and $d$.
    \end{lem*}
    By choosing an appropriate frequency cutoff and then discarding the high frequencies of the input and output of $\cal G^\dag$ one can approximate $\cal G^\dag$ to arbitrary accuracy as shown in the result below.
    
    \begin{lem*}[A.2]
        For any $\epsilon, B > 0$ there exist $w \in \bb N$ such that $\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2(D)} \leq \epsilon$ for all $a \in H^r(D)$ with $\|a\|_{H^r(D)} \leq B$.        
    \end{lem*}
    \begin{proof}
        Using Lemma \ref{lemA1} and stability of $\cal G^\dag$, equation \ref{eq2.1}, we have
        $$
        \begin{aligned}
            \|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2} &\leq \|\cal G^\dag(a) - P_w\cal G^\dag(a)\|_{L^2} + \|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2}
        \end{aligned}
        $$
        Since, $\cal G^\dag : H^r(D;\bb R^{d_{\cal X}}) \to H^s(D;\bb R^{d_{\cal Y}})$, we have $\cal G(a) \in H^s(D;\bb R^{d_{\cal Y}})$. In order to apply Lemma \ref{lemA1}, we need to show that $\cal G^\dag(a) \in C^r(D)$. Note that $H^s(D) \hookrightarrow C^r(D)$ for all $s > r + d/2$ which will imply that $\cal G^\dag (a) \in C^r(D)$. Also, note that $P_w$(Fourier projection operator) is {\it non-expansive} ($L^2$-norm is less than $1$). Taking $\sigma = 0$ in Lemma \ref{lemA1}, we have 
        \begin{equation}            
        \begin{aligned}
            \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} &\leq \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{L^2} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \\
            (\because H^0(D) = L^2(D)) \qquad &= \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{H^0} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2}\\
            (\text{Lemma A.1 and non-expansive}) \qquad &\lesssim w^{-r} \left\|\cal G^\dag (a)\right\|_{H^r} + \left\|\cal G^\dag (a) - \cal G^\dag(P_wa)\right\|_{L^2}\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega(\|a - P_wa\|_{H^\sigma})\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega (Cw^{-(r-\sigma)}\|a\|_{H^r}).
        \end{aligned}
    \end{equation}
    \end{proof}
    Since above relation is true for all $a$ such that $\|a\|_{H^r} \leq B$, it follows that for large enough $w$:
    \begin{equation}
        \sup\limits_{\|a\|_{H^r}\leq B} \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \leq \epsilon.
    \end{equation}
    Given that both $P_wa \in B_w(D)$ and $P_w\cal G^\dag(P_wa) \leq B_w(D)$, a consequence of the above lemma is the existence of an operator $\cal G^* " B_w(D) \to B_w(D):a \mapsto P_w(\cal G^\dag(a))$ that can approximate $\cal G^\dag$ arbitrarily well. Hence $\left\|\cal G^\dag - \cal G^*\right\|_{op} \leq \epsilon$, where the operator are considered as mapping from and to $B_w(D) \cap H^r(D)$ equipped with the $H^r(D)$-norm.

    \subsection{Continuous-Discrete Equivalence for Operator \texorpdfstring{$\cal G^*$}{}} \label{appendix:A.2}
    For every $w > 0$, we denote by $B_w(\bb R^2)$ the space of multivariate bandlimited functions
    $$B_w(\bb R^2) = \{f \in L^2(\bb R^2) : \text{supp}\hat{f} \subset eq [-w,w]^2\},$$
    where $\hat{f}$ denotes the Fourier transform on $L^1(\bb R)$ 
    $$ \hat{f}(\xi) := \int_{\bb R}f(x) e^{-2\pi ix\xi} dx, \qquad \xi \in \bb R,$$
    which extends to $L^2(\bb R)$ by a classical density argument ($L^1(\bb R^n) \cap L^2(\bb R^n)$ is dense in $L^2(\bb R^n), n \geq 1$.) The set $\Psi_w = \{\sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n)\}_{m,n \in \bb Z}$ constitutes an orthonormal basis for $B_w(\bb R^2)$. The bounded operator
    $$ T_{\Psi_w} :l^2(\bb Z^2) \to B_w(\bb R^2), ~~ T_{\Psi_w}(c_{m,n}) = \sum_{m,n\in \bb Z} c_{m,n} \sinc(2w\cdot - m)\cdot \sinc(2w\cdot -n),$$
    which reconstructs a function from its basis coefficients, is called {\it synthesis operator,} and its adjoint 
    $$ T^*_{\Psi_w} : B_w(\bb R^2) \to l^2(\bb Z^2), \qquad T^*_{\Psi_w} f = \left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}_{m,n \in \bb Z},$$
    which extracts basis coefficients from an underlying function, is called {\it analysis operator}.  Every bandlimited function can be uniquely and stably recovered from its sampled values $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$ for ${m,n \in \bb Z}$ via the reconstruction formula
    \begin{equation}
        f(x_1,x_2) = T_{\Psi_w}T^*_{\Psi_w}f(x_1,x_2) = \sum\limits_{m,n \in \bb Z} f\left(\frac{m}{2w},\frac{n}{2w}\right) \sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n),
    \end{equation}
    and we say that there is a {\it continuous-discrete equivalence (CDE)} between $f$ and its samples $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$. In general, every bandlimited function $f \in B_w(\bb R^2)$ cam be uniquely and stably recovered from its sample values $\{f(mT,nT)\}_{m,n\in\bb Z}$ if the {\it sampling rate} or reciprocal of grid size, $1/T$ is greater or equal than the {\it Nyquist rate} $2w$. This simply follows from the fact that $B_w(\bb R^2) \subset B_{w'}(\bb R^2)$ for every $w' > w$. On the contrary, reconstructing $f \in B_w$ at a sampling rate below the Nyquist rate, i.e. $1/T < 2w$, results in a non-zero value for the {\it aliasing error function:}
    $$ \epsilon(f) = f - T_{{\Psi}_{\frac{1}{2T}}} T^*_{{\Psi}_{\frac{1}{2T}}}f,$$
    and the associated {\it aliasing error} $\|\epsilon\|_2$  (see defintion below).
    \begin{defn}[Aliasing for bandlimited functions]
        The aliasing error function $\varepsilon(f)$ and the corresponding aliasing error of $f \in L^2(\bb R)$ for sampling at the rate $2\Omega$ are given by
        $$ \varepsilon(f) = f - \cal P_{B_\Omega}f, \qquad\qquad \|\varepsilon(f)\|_2 = \|f - \cal P_{B_\Omega}f\|_2.$$
        where $\cal P_{B_\Omega} : L^2(\bb R) \to B_\Omega$ is the {\it orthogonal projection operator} onto $B_\Omega$ given by $P_{B_\Omega}f = \sum_{n\in \bb Z}\langle f,\phi_n\rangle\phi_n$. If the aliasing error $\varepsilon(f)$ is zero, i.e. if $f \in B_\Omega$, we say that there is a continuous-discrete equivalence (CDE) between $f$ and its samples $\{f(n/2\Omega)\}_{n \in \bb Z}$.
    \end{defn}
    \noindent Let $\cal G^*$ be a (possibly) non-linear operator between band-limited spaces, i.e. $\cal G^* :B_w(\bb R^2) \to B_{w'}(\bb R^2)$, for some $w,w' > 0$. As given in \ref{appendix:C}, the concepts of continuous-discrete equivalence (CDE) and aliasing error can be adapted to the operator $\cal G^*$. The continuous operator $\cal G^*$ is uniquely determined by a map $\mathfrak{g}_{\Psi_w, \Psi_{w'}}:\ell^2(\bb Z^2) \to \ell^2(\bb Z^2)$ if the aliasing error operator.
    \begin{equation}\label{eq:opaliaserror}
        \varepsilon = \cal G^* - T_{\Psi_{w'}} \circ \mathfrak{g}_{\Psi_w, \Psi_{w'}} \circ T^*_{\Psi_w}
    \end{equation}
    is identically zero, and we say that $\cal G^*$ and $\mathfrak{g}_{\Psi_w, \Psi_{w'}}$, satisfy a continuous-discrete equivalence (Definition \ref{def:opalias}). Equivalently, the diagram \\
    \begin{figure}[!ht]
        \centering
        \begin{tikzcd}
                B_w \arrow[r, "\cal G^*"] \arrow[d, blue, "T^*_{\Psi_w}"] & B_{w'}, \\
                \ell^2(\bb Z^2) \arrow[r, blue, "\mathfrak{g}_{\Psi_w, \Psi_{w'}}"] & \ell^2(\bb Z^2) \arrow[u, blue, "T_{\Psi_{w'}}"]
        \end{tikzcd}
        \caption{}
    \end{figure}

    \noindent commutes, i.e. the black and the blue directed paths in the diagram lead to the same result. In the latter case, since $T^*_{\Psi_w} \circ T_{\Psi_w}$ is identity operator from $\ell^2(\bb Z^2)$ onto itself ($\because \Psi_w$ is taken to be orthonormal basis set, otherwise it is Gram matrix $G_{mn} := \langle \Psi_n,\Psi_m\rangle$), equation \ref{eq:opaliaserror} forces the discretization $\mathfrak{g}_{\Psi_w,\Psi_{w'}},$ to be defined as
    \begin{equation}\label{eq:defg}
        \mathfrak{g}_{\Psi_w,\Psi_{w'}} := T^*_{\Psi_{w'}} \circ \cal G^* \circ T_{\Psi_w}.
    \end{equation}
    The above defintion is motivated from the fact that $\Psi_w$ is orthonormal set and the equation \ref{eq:opaliaserror} when $\varepsilon = 0$.
    \begin{rem}\label{rem:invert}
        Note that in general $T_{\Psi_w}$ is not invertible. When the frame sequence is orthonormal, it is invertible and its inverse is same as its adjoint.
    \end{rem}
    \begin{rem}
        While isolating $\mathfrak{g}_{\Psi_w,\Psi_{w'}}$ on LHS in equation \ref{eq:opaliaserror}, the term $T^{-1}_{\Psi_{w'}}$ will appear and it has been replaced to $T^*_{\Psi_{w'}}$ in equation \ref{eq:defg} because of remark \ref{rem:invert}. Why? That is how we define it. 
    \end{rem}
    \noindent The definition \ref{eq:defg} is equivalent to say that the diagram,
    \begin{figure}[!ht]
        \begin{tikzcd}
            B_w \arrow[r, blue, "\cal G^*"] & B_{w'} \arrow[d, blue, "T^*_{\Psi_{w'}}"], \\
            \ell^2(\bb Z^2) \arrow[r, "\mathfrak{g}_{\Psi_w, \Psi_{w'}}"] \arrow[u, blue, "T_{\Psi_w}"] & \ell^2(\bb Z^2) 
        \end{tikzcd}
        \caption{}
    \end{figure}
    commutes. In other words, once we fix the discrete representation associcated to the input and output functions, there exists a unique way to defined a discretization $\mathfrak{g}_{\Psi_w,\Psi_{w'}}$ that is consistent with the continuous operator $\cal G^*$ and this is given by \ref{eq:defg}.

    \noindent In practice, we may have access to different discrete representations of the input and output functions, e.g. point samples evaluated on different grids, which in the theory amounts to change of reference systems in the functions spaces. For instance, sampling a function $f\in B_w$ on a finer gris $\left\{(\frac{m}{2\overline{w}},\frac{n}{2\overline{w}})\right\}_{m,n \in \bb Z},\overline{w} > w,$ amounts to representing the function $f$ with respect to the system $\Psi_{\overline{w}} = \{\sinc(2\overline{w}x_1 - m) \cdot \sinc(2\overline{w}x_2 - n)\}_{m,n \in \bb Z},$ which constitutes an orthonormal basis for $B_{\overline{w}} \supset B_w$. Then, one can define the associated CDE discretization $\mathfrak{g}_{\Psi_{\overline{w}},\Psi_{\overline{w}'}}$ as in \ref{eq:defg}, and by equation \ref{eq:opaliaserror}, one readily obtains the change of basis formula
    \begin{equation}
        \mathfrak{g}_{\Psi_{\overline{w}},\Psi_{\overline{w}'}} = T^*_{\Psi_{\overline{w}'}} \circ T_{\Psi_{w'}} \circ \mathfrak{g}_{\Psi_w,\Psi_{w'}} \circ T^*_{\Psi_w} \circ T_{\Psi_{\overline{w}}},
    \end{equation}
    Finally, all the above concepts generalize to every pair of frame sequences $(\Psi,\Psi)$ that span respectively the input and output function spaces, and we refer \cite{FB2023} for a complete exposition.
    \section{\bf An Introduction to Frame Theory} \label{appendix:B}
    \noindent Let $\cal H$ be a separable Hilbert space with inner product $\langle\cdot,\cdot\rangle$ and norm $\|\cdot\|$.
    \begin{defn}[Frame]
        A countable sequence of vectors $\{f_i\}_{i\in I}$ in $\cal H$ is a {\it frame} for $\cal H$ if there exists constants $A,B > 0$ such that for all $f \in \cal H$
        $$ A\|f\|^2 \leq \sum_{i\in I} |\langle f,f_i\rangle|^2 \leq B\|f\|^2.$$
        We say that $\{f_i\}_{i\in I}$ is a {\it tight frame} if $A = B$ and, in particular, a {\it Parseval fram} if $A = B = 1$. 
    \end{defn}
    \begin{rem}
        The condition in the above definition ensure that $f$ can be stably reconstructed from its inner products with the frame elements. Stable reconstruction here implies the well-posedness and bounded in both direction (no explosion or vanishing of norm).
    \end{rem}
    \begin{rem}
        A frame is complete in $\cal H$, stable spanning set of $\cal H$.
    \end{rem}
    Clearly by the Parseval identity, an orthonormal basis for $\cal H$ is a Parseval frame ($\because \|f\|^2 = \sum_{n=1}^{\infty}|\langle x,x_n\rangle|^2$, by Parseval's identity for a separable Hilbert space, the so called generalization of the Pythagorean theorem). The lower inequality implies that 
    $$ \langle f,f_i\rangle = 0, \forall i \in I \implies f = 0,$$
    which is equivalent to 
    $$ \overline{\text{span}}\{f_i : i \in I\} = \cal H.$$
    On the other hand the upper inequality implies that the operator
    $$ T:l^2(I) \to \cal H, \qquad T(\{c_i\}_{i\in I}) = \sum_{i\in I} c_if_i,$$
    is bounded with $\|T\| \leq \sqrt{B}$ (\cite{Christensen2008-gk} Theorem 3.1.3), and we call $T$ the {\it synthesis operator}. Its adjoint is given by 
    $$ T^* : \cal H \to \cal H, \qquad Sf = TT^*f = \sum_{i\in I} \langle f,f_i\rangle f_i,$$
    which is bounded, invertible, self-adjoint and positive operator (\cite{Christensen2008-gk}, Lemma 5.1.6). We note that the frame operator is invertible since it is bounded, being a composition of two bounded operator, and the frame property implies that $\|\text{Id} - B^{-1}S\| < 1$, where Id denotes the identity operator. Furthermore, the pseudo-inverse of the synthesis operator is given by
    $$ T^\dag : \cal H \to l^2(I), \qquad T^\dag f = (\langle f, S^{-1}f_i\rangle)_{i\in I},$$
    (\cite{Christensen2008-gk}, Theorem 5.3.7) and $\|T^\dag\| \leq 1/\sqrt{A}$(\cite{Christensen2008-gk}, Proposition 5.3.8). The composition $TT^\dag$ gives the identity operator on $\cal H$, and consequently every element in $\cal H$ can be reconstructed via the reconstruction formula
    \begin{equation}\label{formula}
        f = TT^\dag f= \sum_{i\in f}\langle f,S^{-1}f_i\rangle f_i = \sum_{i\in I}\langle f,f_i\rangle S^{-1}f_i,
    \end{equation}
    where the series converge unconditionally. Formula \ref{formula} is known as the {\it frame decomposition thoerem} (\cite{Christensen2008-gk}, Theorem 5.1.7). In particular, if $\{f_i\}_{i\in I}$ is a tight frame, then $S = A\text{Id}$ and formula \ref{formula} simply reads
    $$ f = \frac{1}{A}\sum_{i\in I}\langle f,f_i\rangle f_i.$$
    On the other hand, the composition $T^\dag T$ gives the orthogonal projection of $l^2(I)$ onto Ran($T^\dag$)(\cite{Christensen2008-gk}, Lemma 2.5.2).

    \noindent In what follows, we consider sequences which are not complete in $\cal H$, and consequently are not frames for $\cal H$, but they are frames for their closed linear span.
    \begin{defn}[Frame sequence]
        Let $\{v_i\}_{i\in I}$ be a countable sequence of vectors in $\cal H$. We say that $\{v_i\}_{i\in I}$ is a frame sequence {\it if it is a frame for $\overline{span}\{v_i:i\in I\}$.}
    \end{defn}
    \begin{rem}
        A frame sequence may not span all of $\cal H$, but it is a frame for the subspace of $\cal H$ which it does span.
    \end{rem}
    \noindent A frame sequence $\{v_i\}_{i\in I}$ in $\cal H$ with synthesis operator $T : l^2(I) \to \overline{span}\{v_i : i\in I\}$ is a frame for $\cal H$ if and only if $T^*$ is injective, whilst in general $T^*$ is not surjective and consequently $T$ is not injective. We denote $\cal V = \overline{span}\{v_i : i\in I\}$. Then, the orthogonal projection of $\cal H$ onto $\cal V$ is given by
    $$ \cal P_{\cal V}f = TT^\dag = \sum_{i\in I}\langle f,S^{-1}v_i\rangle v_i,$$
    where $S : \cal V \to \cal V$ denotes the frame operator. Hence, reconstruction formula \ref{formula} holds if and only if $f\in \cal V$.
    \subsection{\texorpdfstring{Condition $u:\text{Ran }T^\dag_\Psi \to \text{Ran }T^\dag_\Phi$}{}} \label{appendix:B.1}

    Once the discretization is choosen in the input and output spaces, which amounts to choosing a frame pair $(\Psi \subseteq \cal H,\Phi \subseteq \cal K)$, we want to define a mapping $u : \ell^2(I) \to \ell^2(K)$ which handles such discrete representations. By frame decomposition theorem \ref{formula}, every function in $\cal H$ is uniquely determined by a sequence in $\text{Ran } T^\dag_\Phi$, and analogously every function in $\cal K$ is uniquely determined by a sequence in $\text{Ran }T^\dag_\Phi$. Hence, it is sufficient to define a mapping from $\text{Ran }T^\dag_\Psi$ into $\text{Ran }T^\dag_\Phi$. We ensure that when two different discretization both yield zero aliasing, the change of frame formula holds, and thus the representation equivalence error is zero.



    \section{\bf Alias-Free Framework for Operator Learning} \label{appendix:C}
    \noindent In this section, we will extend this concept of aliasing of function to operators. Let $U$ be a operator mapping between infinite-dimensional function spaces and $u$ be a discrete representation mapping.
    
    \paragraph{\bf Setting}Let $U: \text{Dom }U \subseteq \cal H \to \cal K$ be an operator between two separable Hilbert spaces, and let $\Psi = \{\psi_i\}_{i\in I}$ and $\Psi = \{\phi_k\}_{k \in K}$ be frame sequences for $\cal H$ and $\cal K$, respectively, with synthesis operators $T_\Psi$ and $T_\Phi$. We denote their closed linear spans by $\cal M_\Psi := \overline{\text{span}}\{\psi_i : i \in I\}$ and $\cal M_\Phi := \overline{\text{span}}\{\phi_i : k \in K\}$. We note that by classical frame theory \cite{Christensen2008-gk}, the pseudo-inverses $T_\Psi^\dag$ and $T_\Phi^\dag$, initially defined on $\cal M^\Psi$ and $\cal M_\Psi$, respectively, can in fact be extended to the entire Hilbert spaces, i.e. $T_\Psi^\dag : \cal H \to l^2(I)$ and $T_\Phi^\dag : \cal K \to l^2(K)$ and this extension is given explicitly via the dual frame analysis operator.
    \subsection{\bf Operator Aliasing and Representation Equivalence}
    Once the discretization is chosen -- which is equivalent to choosing the frame sequences for input and output spaces $(\Psi,\Phi)$ -- connecting the continuous operator $U$ with its discrete counterpart $u$ is the notion of operator aliasing. Given any mapping $u : l^2(I) \to l^2(K)$, we can build the operator $T_\Phi \circ u \circ T_\Psi^\dag : \cal H \to \cal K$, whose definition clearly on the choices of the frame sequences that we make on the continuous level. In other words, any mapping $u$ can be interpreted as a discrete representation of an underlying continuous operator, which in general, may differ from the operator $U$, that is of interest here. Hence, in analogy to definition of aliasing for functions in Hilbert space, we can define the aliasing error of $U$ relative to the discrete representation $u$ as,
    \begin{defn}[Operator aliasing]\label{def:opalias}
        The aliasing error operator $\varepsilon(U,u,\Psi,\Phi):\text{Dom }U \subseteq \cal H \to \cal K$ is given by
        $$ \varepsilon(U,u,\Psi,\Phi) = U - T_\Phi \circ u \circ T_\Psi^\dag,$$
        and the corresponding scalar error is $\|\varepsilon(U, u, \Psi, \Phi)\|$, with $\|\cdot\|$ denoting the operator norm.
    \end{defn}
    \noindent An aliasing error of zero implies that the operator $U$ can be perfectly represented by first discretizing the function with $T_\Psi^\dag$, applying $u$, and then reconstructing with $T_\Phi$, or equivalently, that the diagram in Figure \ref{fig:3A} commutes, i.e. black and blue directed paths in the diagram lead to the same result. If the aliasing error is zero, we say that $(U, u, \Psi, \Phi)$ satisfies a {\it continuous-discrete equivalence (CDE)}, implying that accessing the discrete representation $u$ is exactly same as accessing the underlying continuous operator $U$.
    \begin{figure}[!ht]
        \centering
        \begin{subfigure}{0.22\textwidth}
            \centering
            \begin{tikzcd}
                \cal H \arrow[r, "U"] \arrow[d, blue, "T_\Psi^\dag"] & \cal K \\
                \ell^2(I) \arrow[r, blue, "u"] & \ell^2(K) \arrow[u, blue, "T_\Phi"]
            \end{tikzcd}
            \caption{An alias-free operator's diagram.}
            \label{fig:3A}
        \end{subfigure} \hfill
        \begin{subfigure}{0.28\textwidth}
            \begin{tikzcd}
                \cal H \arrow[r, blue, "U"] & \cal K \arrow[d, blue, "T_\Phi^\dag"]\\
                \ell^2(I) \arrow[u, blue, "T_\Psi"] \arrow[r, "u"] & \ell^2(K) 
            \end{tikzcd}
            \caption{ReNO: alias-free $u$ can be constructed by discr the operator $U$ for different discretization given by $\Psi,\Phi$.}
            \label{fig:3B}
        \end{subfigure}\hfill
        \begin{subfigure}{0.39\textwidth}
            \begin{tikzcd}
                & \ell^2(I) \arrow[r, blue, "{u(\Psi,\Phi)}"] & \ell^2(K) \arrow[rd, "\textcolor{blue}{T_\Phi}", blue] & \\
                \mathcal{H} \arrow[ru, blue, "T_\Psi^\dagger"] \arrow[rrr, "U"] & & & \mathcal{K} \arrow[ld, "T_{\Phi'}^\dagger", blue]  \\
                & \ell^2(I') \arrow[r, "{u(\Psi',\Phi')}"] \arrow[lu, "\textcolor{blue}{T_{\Psi'}}", blue] & \ell^2(K') &
            \end{tikzcd}
            \caption{ReNO: discrete representations $u,u'$ for different discretizations are equivalent}
            \label{fig:3C}
        \end{subfigure}
        \caption{Alias-free framework}
    \end{figure}
    In practice, the discrete representation $u$ of the operator $U$ depends on the choice of the frame sequences. This means that -- as mentioned at the beginning of the section -- there is one map for every input/output frame sequence $\Phi,\Psi$. The consistency between operations $u,u'$ with respect to different frame sequences can be evaluated using the following error.
    \begin{defn}[Representation equivalence error] Suppose that $u,u'$ are discrete maps with associated frame sequences $\Psi,\Phi$ and $\Psi',\Phi'$, respectively. Then, the representation equivalence error is given by the function $\tau(u,u') : \ell^2(I) \to \ell^2(K),$ defined as:
    $$ \tau(u,u') = u - T^\dag_\Phi \circ T_{\Phi'} \circ u' \circ T_{\Psi'}^\dag \circ T_\Psi$$
    and the corresponding scalar error is $\|\tau(u,u')\|$.
    \end{defn}
    \noindent Intuitively, this amount to computing each mapping on their given discretization, and comparing them by expressing $u'$ in the frames associated to $u$
    \begin{rem}\label{change_frame}
        When there is no aliasing then the representation equivalence error is $0$ and the change of frame formula holds (equate $\tau(u,u')$ to $0$ to get the change of frame formula.)
    \end{rem}

    \subsection{Representation equivalent Neural Operator (ReNO)}
    We now introduce the concept of R{\it Representation equivalent Neural Operator (ReNO)}. To this end, for any pair $(\Psi,\Phi)$ of frame sequences for $\cal H$ and $\cal K$, we consider a mapping at the discrete level $u(\Psi,\Phi) : \text{Ran}T^\dag_\Psi \to \text{Ran}T^\dag_\Phi,$ which handles the discrete representations of the functions. This map is discretization dependent and changes with the choices of frame sequences. When this is clear from the context, we will refer to $u(\Psi,\Phi)$ simply as $u$. See \ref{appendix:B.1} for the explanation of why $\text{Ran }T^\dag_\Psi$ or $\text{Ran }T^\dag_\Phi$ is choosen as domain and codomain respectively.

    \begin{defn}[Representation equivalent Neural Operators (ReNO)] \label{def:ReNO}
        We say that $(U,u)$ is a ReNO if for every pair $(\Psi,\Phi)$ of frame sequences that satisfy $\text{Dom }U \subseteq \cal M_\Psi$ and $\text{Ran }U \subseteq \cal M_\Phi$, there is no aliasing, i.e. the aliasing error operator is identical to zero:
        \begin{equation}
            \varepsilon (U,u,\Psi,\Phi) = 0
        \end{equation} 
        We will write this property in short as $\varepsilon(U,u) = 0$.
    \end{defn}
    \noindent In other words, the diagram in \ref{fig:3A} commutes for every every pair of $(\Psi,\Phi)$. In this case, the discrete representations $u(\Psi,\Phi)$ are all equivalent, meaning that they uniquely determine the same underlying operator $U$, whenever a continuous-discrete equivalence property holds at the level of the function spaces. The domain and range conditions in above definition simply imply that the frame can adequately represent input and output functions of $U$.

    \begin{rem} \label{rem:frame_change}
        If aliasing error $\varepsilon(U,u,\Psi,\Phi) = 0$ then the assumption that $u(\Psi,\Phi)$ maps $\text{Ran } T^\dag_\Psi \subseteq \ell^2(I)$ into $\text{Ran } T^\dag_\Phi \subseteq \ell^2(K)$ implies that 
        \begin{equation}
            u(\Psi,\Phi) = T^\dag_{\Phi} \circ U \circ T_\Psi.
        \end{equation}
        We observe that this definition of $u(\Psi,\Phi)$ is such that the diagram in Figure \ref{fig:3B} commutes. In other words, once we fix the discrete representation associated to the input and output functions, there exists a unique way to defined a discretization $u(\Psi,\Phi)$ that is consistent with the continuous operator $U$ and this is given by the above equation. 
        \begin{proof}
            If aliasing error $\varepsilon(U,u,\Psi,\Phi) = 0$, then
            \begin{equation}
                U = T_\Phi \circ u(\Psi,\Phi) \circ T^\dag_\Psi.
            \end{equation}
            By above equation, we obtain
            $$ T^\dag_\Phi \circ U \circ T_\Psi = T^\dag_\Phi \circ T_\Phi \circ u(\Psi,\Phi) \circ T^\dag_\Psi \circ T_\Psi = u(\Psi,\Phi).$$
            where the last equality follows from the fact that $T^\dag_\Psi \circ T_\Psi$ is the orthogonal projection onto $(\text{Ker}(T_\Psi))^{\perp} = \text{Ran}(T^\dag_\Psi)$ and, by assumption, $u(\Psi,\Phi)$ maps $\text{Ran }T^\dag_\Psi$ into $\text{Ran }T^\dag_\Phi$.
        \end{proof}
    \end{rem}

    \noindent In partiuclar, Remark \ref{rem:frame_change} directly implies the formula to go from one discrete representation to another, as
    \begin{equation} \label{eq:frame_change}
        u(\Psi',\Phi') = T^\dag_{\Psi'} \circ T_\Phi \circ u(\Psi,\Phi) \circ T^\dag_\Psi \circ T_{\Psi'}
    \end{equation}
    whenever the pairs of frame sequences $(\Psi,\Phi)$ and $(\Psi',\Psi')$ satisfy the conditions in Definition \ref{def:ReNO}. In other words, the diagram in \ref{fig:3C} commutes.

    \begin{prop}[Equivalence of ReNO discrete representations]
        Let $(U,u)$ be a ReNO. For any two frame sequence pairs $(\Psi,\Phi)$ and $(\Psi',\Phi')$ satisfying conditions in Definition \ref{def:ReNO}, we have that 
        $$ \tau(u,u') = 0,$$
        where, by a slight abuse of notation, $u'$ denotes $u(\Psi',\Phi')$.
    \end{prop}
    \begin{proof}
        Immediately implied by the formula \ref{eq:frame_change}. This proposition establishes the link between aliasing and representation equivalence i.e. if aliasing is $0$ then it will imply the representation equivalence error to be $0$.
    \end{proof}


\bibliographystyle{plainnat}
\bibliography{ref_cno}
\end{document}