%% Credits of this ams template are with respective people. @Devansh1106 neither own this template nor the credits. 
\documentclass[reqno,10pt]{amsart}
\usepackage[a4paper, margin=1.25in]{geometry} % Change 1in to desired size
\usepackage[numbers]{natbib}
\setlength{\bibsep}{8pt}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lineno}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{esint}
\usepackage{fancyhdr}
\setlength{\parskip}{0.5\baselineskip}%
\setlength{\parindent}{20pt}
\theoremstyle{plain}

\pagestyle{fancy}
\fancyhf{} % Clear default headers/footers
\setlength{\headheight}{10.0pt}
% Even pages: Paper title
\fancyhead[LE]{\footnotesize\textit{Nonlocality and Nonlinearity Implies Universality}}

% Odd pages: Author name
\fancyhead[RO]{\footnotesize\textit{Devansh Tripathi}}

\newtheorem*{thm*}{Theorem}
%% this allows for theorems which are not automatically numbered
\newcommand{\sinc}{\text{sinc}}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{rem}{Remark}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cal}[1]{\mathcal{#1}}

\hypersetup{
    colorlinks=false,
    linkcolor=blue,    % Internal links (sections, equations)
    citecolor=red,     % Citation links
    urlcolor=magenta   % URLs (DOIs, websites)
}
%% The above lines are for formatting.  In general, you will not want to change these.

\title{Report on the paper ``Convolutional Neural Operator for robust and accurate learning of PDEs''}
\author{Devansh Tripathi$^1$ \\ ETH Z\lowercase{\"urich}}
\thanks{$^1$Seminar für Angewandte Mathematik, HG E 62.2, Rämistrasse 101, 8092 Zürich, Switzerland \\ \href{mailto:devansh.tripathi@sam.math.ethz.ch}{\texttt{devansh.tripathi@sam.math.ethz.ch}}}

\begin{document}
\numberwithin{equation}{section}

\begin{abstract}
    In the paper \cite{BR2023}, the author argues that the convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ingnored in the context of learning solution operators of PDEs. The author present a novel framework termed as convolutional neural operators (CNOs) that is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. The author also proved a universal approximation result for CNOs.
\end{abstract}
\maketitle
\section{\bf \large Introduction}
    Given the ubiquitous nature of partial differential equations (PDEs) as mathematical models in the science and engineering, it becomes important to develop method to approximate the solutions to a PDE with less computational cost. There are well-established numerical methods such as finite differences, finite elements, finite volumes and spectral methods that have been successfully used to approximate PDE solution operator. Hwoever, the high computational cost of these methods, particularly in high dimensions and for {\it many query} problems such Uncertainity Quantification (UQ), inverse problems etc. calls upon the design of {\it fast, robus and accurate} surrogates.

    \noindent As {\it operators} are the objects of interest in solving PDEs, learning such operators from data which is loosely termed as {\it operator learning}, has emerged as a dominant paradigm in recent years. As it is argued in a recent paper \cite{FB2023}, a structure-preserving operator learning algorithm or {\it representation equivalent neural operator} has to respect some form of continuous-discrete equivalence (CDE) in order to learn the underlying operator, rather than just a discrete representation of it. Failure to respect such a CDE can lead to the so-called aliasing errors \cite{FB2023} and affect model performance at multiple discrete resolutions.

    \noindent The naive use of convolutional neural networks (CNNs) in the context of operator learning, see \cite{FB2023,Zhu2018,ZL2021} on how using CNNs for operator learning leads to results that heavily rely on the underlying grid resolution. The author has made the following contributions in this paper:
    \begin{itemize}
        \item The author proposes novel modifications to CNNs in order to enforce structure-preserving continuous-discrete equivalence (CDE) and enable the genuine, alias-free, learning of operators. The  resulting architecture, termed as {\it Convolutional Neural Operator}(CNO), is provided as novel {\it operator} adaptation of the widely used U-Net architecture.
        \item The author has shown that CNO is a {\it representation equivalent neural operator} in the sense of \cite{FB2023}, and also proved a universality result for CNOs to any desired accuracy.
        \item CNO has been tested on a {\it novel} set of benchmarks, known as {\it Representative PDE Benchmarks}(RPB), that span across a variety of PDEs ranging from linear elliptic and hyperbolic to nonlinear parabolic and hyperbolic PDEs, with possibly {\it multiscale solutions}. 
    \end{itemize}

    \section{Convolutional Neural Operator}
    \paragraph{\bf Setting} For simplicity, we will focus here on the two-dimensional case by specifying the underlying domain as $D = \bb T^2$, being the $2$-d torus. Let $\cal X = H^r(D,\bb R^{d_{\cal X}}) \subset \cal Z$ and $\cal Y = H^s(D,\bb R_{d_\cal Y})$ be the underlying function spaces, where $H^{r,s}(D,\cdot)$ are sobolev spaces of order $r$ and $s$. Without loss of generality, we set $r = s$ hereafter. Our aim would be to aproximate {\it continuous operators} $\cal G^\dag : \cal X \to \cal Y$ from data pairs $(u_i, \cal G^\dag(u_i))_{i=1}^M \in \cal X \times \cal Y$. We furthur assume that there exists a {\it modulus of continuity} for the operator i.e.,
    \begin{equation}\label{eq2.1}
        \|\cal G^\dag(u) - \cal G^\dag(v)\|_{\cal Y} \leq \omega(\|u - v\|_{\cal Z}), \qquad \forall u,v \in \cal X,
    \end{equation}
    with $\omega : \bb R_+ \to \bb R_+$ being a monotonically increasing function with $\lim_{y\to 0}\omega(y) = 0$ (implies that the operator $\cal G^\dag$ is uniformly continuous) The underlying operator $\cal G^\dag$ can corresponds to solution operators for PDEs but is more general that that and encompasses examples such as those arising in inverse problems, for instance in imaging.
    \paragraph{\bf Bandlimited Approximation} As argued in the paper \cite{FB2023} that Sobolev spaces such as $H^r$ are, in a sense, too large to allow for any {\it continuous-discrete equivalence} (CDE), i.e. equivalence between the underlying operator and its discrete representations, which is necessary for robust operator learning. We have to consider small subspaces of $H^r$ which allow for such CDEs. We choose the space of {\it bandlimited functions} defined by,
    \begin{equation}
        B_w(D) = \{f \in L^2(D) : \text{supp}\hat{f} \subseteq [-w,w]^2\},
    \end{equation}
    for some $w > 0$ and with $\hat{f}$ denoting the Fourier transform of $f$. From 

    \appendix
    \section{Detailed proofs} \label{appendix:A}
    \subsection[A.1]{Approximation of Operators mapping between Sobolev spaces by operators mapping between spaces of bandlimited functions}
    We prove that one can approximate any continuous operator $\cal G^\dag : \cal X \to \cal Y$ by an operator mapping between spaces of bandlimited functions to arbitrary accuracy. We obtain this result by dsicarding the high-frequency components, higher than the frequency $w$, of both the input and the output of $\cal G^\dag$ by a Fourier projection $P_w$. For orthogonal Fourier projections and also trigonometric polynomial interpolation, the following result on the accuracy of the projection holds,

    \begin{lem*}[A.1] \label{lemA1}
        Given $\sigma, r \in \bb N_0$ with $r > d/2$ and $r \geq \sigma$, and $f \in C^r(\bb T^d)$ it holds for every $w \in \bb N$ that, 
        \begin{equation}
            \|f - P_w(f)\|_{H^\sigma(\bb T^d)} \leq C(r,d)w^{-(r-\sigma)}\|f\|_{H^r(\bb T^d)},
        \end{equation}
        for a constant $C(r,d) > 0$ that only depends on $r$ and $d$.
    \end{lem*}
    By choosing an appropriate frequency cutoff and then discarding the high frequencies of the input and output of $\cal G^\dag$ one can approximate $\cal G^\dag$ to arbitrary accuracy as shown in the result below.
    
    \begin{lem*}[A.2]
        For any $\epsilon, B > 0$ there exist $w \in \bb N$ such that $\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2(D)} \leq \epsilon$ for all $a \in H^r(D)$ with $\|a\|_{H^r(D)} \leq B$.        
    \end{lem*}
    \begin{proof}
        Using Lemma \ref{lemA1} and stability of $\cal G^\dag$, equation \ref{eq2.1}, we have
        $$
        \begin{aligned}
            \|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2} &\leq \|\cal G^\dag(a) - P_w\cal G^\dag(a)\|_{L^2} + \|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2}
        \end{aligned}
        $$
        Since, $\cal G^\dag : H^r(D;\bb R^{d_{\cal X}}) \to H^s(D;\bb R^{d_{\cal Y}})$, we have $\cal G(a) \in H^s(D;\bb R^{d_{\cal Y}})$. In order to apply Lemma \ref{lemA1}, we need to show that $\cal G^\dag(a) \in C^r(D)$. Note that $H^s(D) \hookrightarrow C^r(D)$ for all $s > r + d/2$ which will imply that $\cal G^\dag (a) \in C^r(D)$. Also, note that $P_w$(Fourier projection operator) is {\it non-expansive} ($L^2$-norm is less than $1$). Taking $\sigma = 0$ in Lemma \ref{lemA1}, we have 
        \begin{equation}            
        \begin{aligned}
            \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} &\leq \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{L^2} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \\
            (\because H^0(D) = L^2(D)) \qquad &= \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{H^0} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2}\\
            (\text{Lemma A.1 and non-expansive}) \qquad &\lesssim w^{-r} \left\|\cal G^\dag (a)\right\|_{H^r} + \left\|\cal G^\dag (a) - \cal G^\dag(P_wa)\right\|_{L^2}\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega(\|a - P_wa\|_{H^\sigma})\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega (Cw^{-(r-\sigma)}\|a\|_{H^r}).
        \end{aligned}
    \end{equation}
    \end{proof}
    Since above relation is true for all $a$ such that $\|a\|_{H^r} \leq B$, it follows that for large enough $w$:
    \begin{equation}
        \sup\limits_{\|a\|_{H^r}\leq B} \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \leq \epsilon.
    \end{equation}
    Given that both $P_wa \in B_w(D)$ and $P_w\cal G^\dag(P_wa) \leq B_w(D)$, a consequence of the above lemma is the existence of an operator $\cal G^* " B_w(D) \to B_w(D):a \mapsto P_w(\cal G^\dag(a))$ that can approximate $\cal G^\dag$ arbitrarily well. Hence $\left\|\cal G^\dag - \cal G^*\right\|_{op} \leq \epsilon$, where the operator are considered as mapping from and to $B_w(D) \cap H^r(D)$ equipped with the $H^r(D)$-norm.

    \subsection{Continuous-Discrete Equivalence for Operator \texorpdfstring{$\cal G^*$}{}}
    For every $w > 0$, we denote by $B_w(\bb R^2)$ the space of multivariate bandlimited functions
    $$B_w(\bb R^2) = \{f \in L^2(\bb R^2) : \text{supp}\hat{f} \subset eq [-w,w]^2\},$$
    where $\hat{f}$ denotes the Fourier transform on $L^1(\bb R)$ 
    $$ \hat{f}(\xi) := \int_{\bb R}f(x) e^{-2\pi ix\xi} dx, \qquad \xi \in \bb R,$$
    which extends to $L^2(\bb R)$ by a classical density argument ($L^1(\bb R^n) \cap L^2(\bb R^n)$ is dense in $L^2(\bb R^n), n \geq 1$.) The set $\Psi_w = \{\sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n)\}_{m,n \in \bb Z}$ constitutes an orthonormal basis for $B_w(\bb R^2)$. The bounded operator
    $$ T_{\Psi_w} :l^2(\bb Z^2) \to B_w(\bb R^2), ~~ T_{\Psi_w}(c_{m,n}) = \sum_{m,n\in \bb Z} c_{m,n} \sinc(2w\cdot - m)\cdot \sinc(2w\cdot -n),$$
    which reconstructs a function from its basis coefficients, is called {\it synthesis operator,} and its adjoint 
    $$ T^*_{\Psi_w} : B_w(\bb R^2) \to l^2(\bb Z^2), \qquad T^*_{\Psi_w} f = \left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}_{m,n \in \bb Z},$$
    which extracts basis coefficients from an underlying function, is called {\it analysis operator}.  Every bandlimited function can be uniquely and stably recovered from its sampled values $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$ for ${m,n \in \bb Z}$ via the reconstruction formula
    \begin{equation}
        f(x_1,x_2) = T_{\Psi_w}T^*_{\Psi_w}f(x_1,x_2) = \sum\limits_{m,n \in \bb Z} f\left(\frac{m}{2w},\frac{n}{2w}\right) \sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n),
    \end{equation}
    and we say that there is a {\it continuous-discrete equivalence (CDE)} between $f$ and its samples $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$. In general, every bandlimited function $f \in B_w(\bb R^2)$ cam be uniquely and stably recovered from its sample values $\{f(mT,nT)\}_{m,n\in\bb Z}$ if the {\it sampling rate} or reciprocal of grid size, $1/T$ is greater or equal than the {\it Nyquist rate} $2w$.
\bibliographystyle{plainnat}
\bibliography{ref_cno}
\end{document}