%% Credits of this ams template are with respective people. @Devansh1106 neither own this template nor the credits. 
\documentclass[reqno,10pt]{amsart}
\usepackage[a4paper, margin=1.25in]{geometry} % Change 1in to desired size
\usepackage[numbers]{natbib}
\setlength{\bibsep}{8pt}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{subcaption}
\captionsetup[subfigure]{labelformat=parens,labelfont=normalfont}
\renewcommand{\thesubfigure}{\alph{subfigure}}
\usepackage{hyperref}
\usepackage{lineno}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{esint}
\usepackage{fancyhdr}
\setlength{\parskip}{0.5\baselineskip}%
\setlength{\parindent}{20pt}
\theoremstyle{plain}

\pagestyle{fancy}
\fancyhf{} % Clear default headers/footers
\setlength{\headheight}{10.0pt}
% Even pages: Paper title
\fancyhead[LE]{\footnotesize\textit{Universality for CNOs in setting of ANOs}}

% Odd pages: Author name
\fancyhead[RO]{\footnotesize\textit{Devansh Tripathi}}

\newtheorem*{thm*}{Theorem}
%% this allows for theorems which are not automatically numbered
\newcommand{\sinc}{\text{sinc}}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{rem}{Remark}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cal}[1]{\mathcal{#1}}

\hypersetup{
    colorlinks=false,
    linkcolor=blue,    % Internal links (sections, equations)
    citecolor=red,     % Citation links
    urlcolor=magenta   % URLs (DOIs, websites)
}
%% The above lines are for formatting.  In general, you will not want to change these.

\title{Report on the paper ``Convolutional Neural Operator for robust and accurate learning of PDEs''}
\author{Devansh Tripathi$^1$ \\ ETH Z\lowercase{\"urich}}
\thanks{$^1$Seminar für Angewandte Mathematik, HG E 62.2, Rämistrasse 101, 8092 Zürich, Switzerland \\ \href{mailto:devansh.tripathi@sam.math.ethz.ch}{\texttt{devansh.tripathi@sam.math.ethz.ch}}}

\begin{document}
\numberwithin{equation}{section}

\begin{abstract}
    In the paper \cite{BR2023}, the author argues that the convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ingnored in the context of learning solution operators of PDEs. The author present a novel framework termed as convolutional neural operators (CNOs) that is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. The author also proved a universal approximation result for CNOs.
\end{abstract}
\maketitle
\section{\bf \large Introduction}
    Given the ubiquitous nature of partial differential equations (PDEs) as mathematical models in the science and engineering, it becomes important to develop method to approximate the solutions to a PDE with less computational cost. There are well-established numerical methods such as finite differences, finite elements, finite volumes and spectral methods that have been successfully used to approximate PDE solution operator. However, the high computational cost of these methods, particularly in high dimensions and for {\it many query} problems such Uncertainity Quantification (UQ), inverse problems etc. calls upon the design of {\it fast, robust and accurate} surrogates.

    \noindent As {\it operators} are the objects of interest in solving PDEs, learning such operators from data which is loosely termed as {\it operator learning}, has emerged as a dominant paradigm in recent years. As it is argued in a recent paper \cite{FB2023}, a structure-preserving operator learning algorithm or {\it representation equivalent neural operator} has to respect some form of continuous-discrete equivalence (CDE) in order to learn the underlying operator, rather than just a discrete representation of it. Failure to respect such a CDE can lead to the so-called aliasing errors \cite{FB2023} and affect model performance at multiple discrete resolutions.

    \noindent The naive use of convolutional neural networks (CNNs) in the context of operator learning, see \cite{FB2023,Zhu2018,ZL2021} on how using CNNs for operator learning leads to results that heavily rely on the underlying grid resolution. The author has made the following contributions in this paper:
    \begin{itemize}
        \item The author proposes novel modifications to CNNs in order to enforce structure-preserving continuous-discrete equivalence (CDE) and enable the genuine, alias-free, learning of operators. The  resulting architecture, termed as {\it Convolutional Neural Operator} (CNO), is provided as novel {\it operator} adaptation of the widely used U-Net architecture.
        \item The author has shown that CNO is a {\it representation equivalent neural operator} in the sense of \cite{FB2023}, and also proved a universality result for CNOs.
        \item CNO has been tested on a {\it novel} set of benchmarks, known as {\it Representative PDE Benchmarks} (RPB), that span across a variety of PDEs ranging from linear elliptic and hyperbolic to nonlinear parabolic and hyperbolic PDEs, with possibly {\it multiscale solutions}. 
    \end{itemize}

    \section{Convolutional Neural Operator}
    \paragraph{\bf Setting} For simplicity, we will focus here on the two-dimensional case by specifying the underlying domain as $D = \bb T^2$, being the $2$-d torus. Let $\cal X = H^r(D,\bb R^{d_{\cal X}}) \subset \cal Z$ and $\cal Y = H^s(D,\bb R_{d_\cal Y})$ be the underlying function spaces, where $H^{r,s}(D,\cdot)$ are sobolev spaces of order $r$ and $s$. Without loss of generality, we set $r = s$ hereafter. Our aim would be to aproximate {\it continuous operators} $\cal G^\dag : \cal X \to \cal Y$ from data pairs $(u_i, \cal G^\dag(u_i))_{i=1}^M \in \cal X \times \cal Y$. We furthur assume that there exists a {\it modulus of continuity} for the operator i.e.,
    \begin{equation}\label{eq2.1}
        \|\cal G^\dag(u) - \cal G^\dag(v)\|_{\cal Y} \leq \omega(\|u - v\|_{\cal Z}), \qquad \forall u,v \in \cal X,
    \end{equation}
    with $\omega : \bb R_+ \to \bb R_+$ being a monotonically increasing function with $\lim_{y\to 0}\omega(y) = 0$ (implies that the operator $\cal G^\dag$ is uniformly continuous) The underlying operator $\cal G^\dag$ can corresponds to solution operators for PDEs but is more general that that and encompasses examples such as those arising in inverse problems, for instance in imaging.
    \paragraph{\bf Bandlimited Approximation} As argued in the paper \cite{FB2023} that Sobolev spaces such as $H^r$ are, in a sense, too large to allow for any {\it continuous-discrete equivalence} (CDE), i.e. equivalence between the underlying operator and its discrete representations, which is necessary for robust operator learning. We have to consider small subspaces of $H^r$ which allow for such CDEs. We choose the space of {\it bandlimited functions} defined by,
    \begin{equation}
        B_w(D) = \{f \in L^2(D) : \text{supp}\hat{f} \subseteq [-w,w]^2\},
    \end{equation}
    for some $w > 0$ and with $\hat{f}$ denoting the Fourier transform of $f$. It is shown in appendix \ref{appendix:A.1} that for any $\varepsilon > 0$, there exists a $w$, large enough depending on $r$, and a continuous operator $\cal G^* : B_w(D) \to B_w(D)$, such that $\|\cal G^\dag - \cal G^*\| \leq \varepsilon,$ with $\|\cdot\|$ denoting the corresponding operator norm. Along with that \ref{appendix:A.2} shows that we can define discrete versions of $\cal G^*$ using the underlying $sinc$ basis for bandlimited functions and establish a continuously-discrete equivalence for it.

    \paragraph{\bf Definition of CNO} \label{def:CNO} Given above context, our goal will be to approximate the operator $\cal G^*$ in a {\it structure-preserving} manner i.e. as the underlying operator maps between spaces of bandlimited functions, we will construct our operator approximation architecture to also map bandlimited functions to bandlimited functions, thus respecting the continuous-discrete equivalence.

    \noindent We denote the operator $\cal G : B_w(D) \to B_w(D)$ as a {\it convolutional neural operator} (CNO) which we define as a compositional mapping between functions as
    \begin{equation}\label{eq2.3}
        \cal G : u \mapsto P(u) = v_0 \mapsto v_1 \mapsto \dots v_L \mapsto Q(v_L) = \overline{u},
    \end{equation}
    where
    \begin{equation}
        v_{l+1} = \cal P_l \circ \Sigma_l \circ \cal K_l(v_l), \qquad 1 \leq l \leq L - 1.
    \end{equation}
    From \ref{eq2.3}, we see that first, the input function $u \in B_w(D)$ is lifted to the latent space of bandlimited functions through a {\it lifting layer}: 
    $$ P : \left\{u \in B_w(D,\bb R^{d_{\cal X}}) \right\} \to \left\{u \in B_w(D,\bb R^{d_0}) \right\}$$ 
    Here, $d_0 > d_{\cal X}$ is the number of channels in the lifted, latent space. The lifting operation is performed by a convolution operator which will be defined below.

    \noindent Then, the lifted function is processed through the composition of a series of mappings between functions (layers), with each layer consisting of three elementary mappings, i.e. $\cal P_l$ is either the {\it upsampling} or {\it downsampling} operator, $\cal K_l$ is the convolution operator and $\Sigma_l$ is the activation operator. Finally, the last output function in the iterative procedure $v_L$ is projected to the output space with a {\it projection operator} $Q$, defined as 
    $$ Q : \left\{v_L \in B_w(D,\bb R^{d_0}) \right\} \to \left\{\overline{u} \in B_w(D,\bb R^{d_{\cal Y}}) \right\}.$$
    The projection operation is also performed by a convolution operator defined below.

    \paragraph{\bf Convolution Operator} For simplicity, {\it single channel} version of the convolution operator $K_l$ is presented here. See appendix \ref{appendix:A.3} for {\it multi-scale} version. Convolution operations are performed with discrete kernals
    $$ K_w = \sum_{i,j=1}^{k} k_{ij} \cdot \delta_{z_{ij}}$$
    defined on a $s \times s$ uniform grid on $D$ with grid size $\geq 1/2w$, in-order to satisfy the requirements of the Whittaker-Shannon Kotelnikov sampling theorem \cite{MU2000}, and $z_{ij}$ being the resulting grid points, $k \in\bb N$ being kernel size and $\delta_x$ denoting the Dirac measure at point $x\in D$. The convolution operator for a {\it single-channel} $\cal K_w : B_w(D) \to B_w(D)$ is defined by 
    $$ \cal K_wf(x) = (K_w \star f)(x) = \int_D K_w(x-y)f(y) dy = \sum_{i,j=1}^{k} k_{i,j} f(x-z_{ij}), \qquad \forall x\in D,$$
    where the last identity arises from the fact that $f \in B_w$. Thus, our convolution operator is directly parameterized in physical space, in contrast to the Fourier space parameterization of convolution in the FNO architecture of \cite{ZL2021}. Hence, our parameterization is of a {\it local} nature.

    \paragraph{\bf Upsampling and Downsampling Operator} For some $\overline{w} > w$, we can {\it upsample} a function $f \in B_w$ to the {\it higher band} $B_{\overline{w}}$ by simple setting,
    \begin{equation}\label{equp}
    \cal U_{w,\overline{w}}: B_w(D) \to B_{\overline{w}}(D), \qquad \cal U_{w,\overline{w}} f(x) = f(x), \qquad \forall x\in D.
    \end{equation}
    On the other hand, for some $\underline{w} < w$, we can {\it downsample} a function $ f\in B_w$ to the {\it lower band} $B_{\underline{w}}$ by setting $\cal D_{w,\underline{w}} : B_w(D) \to B_{\underline{w}}(D),$ defined by 
    \begin{equation}\label{eqdown}
        \cal D_{w,\underline{w}}f(x) = \left(\frac{\underline{w}}{w}\right)^2 (h_{\underline{w}} \star f)(x) = \left(\frac{\underline{w}}{w}\right)^2 \int_D h_{\underline{w}}(x-y) f(y) dy, \qquad \forall x\in D,
    \end{equation}
    where $\star$ is the convolution operation on functions defined above and $h_{\underline{w}}$ is the so-called {\it interpolation sinc filter}:
    \begin{equation}\label{eq2.5}
        h_w(x_0,x_1) = \sinc(2wx_0) \cdot \sinc(2wx_1), \qquad (x_0,x_1) \in \bb R^2.
    \end{equation}

    \paragraph{\bf Activation Layer} Naturally, one can apply the activation function pointwise to any function. However, it is well-known that such an application will no longer respect the band-limits of the underlying function space and generate {\it aliasing error} \cite{FB2023}. In particular, nonlinear activations can generate features at arbitrarily high frequencies. Our aim is to respect the underlying CDE, we will modulate the application of the activation function so that the resulting outputs falls within desired band limits.

    \noindent First, we will upsample the input function $f \in B_w$ to a higher bandlimit $\overline{w} > w$, then apply the activation function pointwise and finally downsample the result back to the original bandlimit $w$. Implicitly assuming that $\overline{w}$ is large enough such that $\sigma(B_w) \subset B_{\overline{w}}$, we define the activation layer as,
    \begin{equation}\label{eqactivation}
        \Sigma_{w,\overline{w}} : B_w(D) \to B_w(D), \qquad \Sigma_{w,\overline{w}} f(x) = \cal D_{\overline{w},w}(\sigma \circ \cal U_{w,\tilde{w}} f)(x), \qquad \forall x\in D.
    \end{equation}

    \paragraph{\bf Instantiation through an Operator U-Net architecture} The above blocks are assembled together in the form of an Operator U-Net architecture that has bandlimited functions as input and outputs. We also need other ingredients such as {\it skip connections} through {\it ResNet} blocks of the form, $\cal R_{w,\overline{w}} : B_w(D,\bb R^d) \to B_w(D,\bb R^d),$ such that
    \begin{equation}
        \cal R_{w,\overline{w}}(v) = v + \cal K_w \circ \Sigma_{w,\overline{w}} \circ \cal K_w(v), \qquad \forall v \in B_w(D,\bb R^d).        
    \end{equation}
    we also need the {\it Invariant blocks} of the form , $\cal I_{w,\overline{w}}:B_w(D,\bb R^d) \to B_w(D,\bb R^d)$ such that 
    \begin{equation}
        \cal I_{w,\overline{w}}(v) = \Sigma_{w,\overline{w}} \circ \cal K_w(v), \qquad \forall v \in B_w(D,\bb R^d).
    \end{equation}

    \paragraph{\bf Continuous-Discrete Equivalence for CNO} We have defined CNO as an operator that maps bandlimited functions to bandlimited functions. In practice, CNO has to be implemented in a discrete manner with {\it discretized versions} of each of the above-defined elementary operations. We prove the following proposition:
    \begin{prop}
        Convolutional Neural Operator $\cal G: B_w(D,\bb R^{d_{\cal X}}) \to B_w(D,\bb R^{d_{\cal Y}})$ is a Representation equivalent neural operator or ReNO, in the sense of Defintion \ref{def:ReNO}.      
    \end{prop}
    \noindent The proof of the above result can be found in the paper (\cite{BR2023}, SM A.5).

    \section{Universal Approximation by CNOs}
    \noindent We consider the following abstract PDE in the domain $D = \bb T^2$,
    \begin{equation}\label{eq:pde}
        \cal L(u) = 0, \qquad \cal B(u) = 0,
    \end{equation}
    with $\cal L$ being the differential operator and $\cal B$ a boundary operator. We assume that the differential operator $\cal L$ only depends on the coordinate $x$ through a {\it coefficient} function $a \in H^r(D).$ The corresponding {\it solution} operator is denoted by $\cal G^\dag : \cal X^* \subset H^r(D) \to H^r(D) : a \mapsto u,$ with $u$ beign the solution of the PDE (\ref{eq:pde}). We assume that $\cal G^\dag$ is continuous. Moreover, we also assume the following modulus of continuity,
    \begin{equation}\label{eq3.2}
        \left\|\cal G^\dag(a) - \cal G^\dag(a')\right\|_{L^p(\bb T^2)} \leq \omega\left(\|a - a'\|_{H^\sigma(\bb T^2)}\right),
    \end{equation}
    for some $p \in \{2,\infty\}$ and $0 \leq \sigma \leq r-1$, and where $\omega : [0\infty) \to [0,\infty)$ is a monotonously increasing function with $\lim_{y \to 0} \omega (y) = 0$. Above equation will be automatically satisfied if $\cal X^*$ is compact and $\cal G^\dag$ is continuous. Under these assumptions, we have the following {\it universality theorem} for CNOs.

    \begin{thm}
        Let $\sigma \in \bb N_0$ and $p \in \{2,\infty\}$ as in (\ref{eq3.2}), $r > \max{\sigma, 2/p}$ and $B > 0$. For any $\varepsilon > 0$ and any operator $\cal G^\dag$, as defined above, there exists a CNO $\cal G$ such that for every $a \in \cal X^*$ with $\|a\|_{H^r(D)} \geq B$ it holds,
        \begin{equation}
            \|\cal G^\dag(a) - \cal G(a)\|_{L^p(D)} < \epsilon.
        \end{equation}
    \end{thm}
    \noindent The more direct proof for the above result can be found in the paper (\cite{BR2023}, SM B).
    
    \subsection{Alternate proof using universality of ANOs}
    Here, we will give an alternate proof by showing that CNOs are equivalent to ANO under suitable choice of the kernel and then we refer to the paper \cite{SL2024} for the fact that ANOs are universal approximator, specifically Theorem $2.2$ in the paper \cite{SL2024} which in turn imply that CNOs are universal approximators.

    \begin{proof}
        Recall that the hidden layers of the CNO are of the form
        \begin{equation}
            v_{l+1} = \cal P_l \circ \Sigma_l \circ \cal K_l(v_l), \qquad 1 \leq l \leq L-1
        \end{equation}
        where $\cal K$ is the convolution operator, $\Sigma$ is the activation operator and $\cal P$ is either a downsampling or a upsampling operator. From equation \ref{equp}, \ref{eqdown} and \ref{eqactivation}, we have the final form of the activation layer as 
        \begin{equation}\label{eq:cno_activation}
            v_{l+1} = \cal P_l \circ \cal D_{\overline{w},w} \circ \sigma \circ \cal U_{w,\tilde{w}} \circ \cal K_l(v_l), \qquad 1 \leq l \leq L-1
        \end{equation}
        where the convolution operator $\cal K_l$ is given by integration against a discrete kernel $K_w \in \bb R^{k \times k}$, 
        $$  K_w = \sum_{i,j = 1}^{k} k_{ij} \cdot \delta_{z_{ij}}, \qquad \cal K_wv(x) = (K_w \star v)(x) = \sum_{i,j=1}^{k}k_{ij}v(x-z_{ij}), \quad\forall x \in D$$
        where $k \in \bb N$ is the kernel size, $\delta_x$ is the dirac measure at $x \in D$ and $z_{ij}$ are the grid points. Without loss of generality, let us assume that
        \begin{equation}\label{eq:kernel_cno}
            k_{ij}= |D|^{-1}\left(\frac{1}{N}\right)^2
        \end{equation}
        for all $i,j \in \bb N$ and $N$ being the total number of grid points as given in the next paragraph. Then we have the following form of convolution operator
        \begin{equation}\label{eq:convop}
            \cal K_wv(x) = \frac{1}{|D|}\left(\frac{1}{N}\right)^2 \sum_{i=1}^{k} \sum_{j=1}^{k} v(x-z_{ij}), \qquad x \in D
        \end{equation}
        In order to show that above is equivalent to nonlocality in ANO, we need to show that nonlocality (\ref{eq:convop}) is of the form
        \begin{equation}\label{eq:ano_nonlocality}
            \fint_D v(y)dy = \frac{1}{|D|} \int_D v(y)dy.
        \end{equation}
        For this purpose, we use the properties of the space to which the function $v$ belongs to i.e. bandlimited space. The problem now can be reduced to show that how accurately we can represent this integral by the summation involved in \ref{eq:convop}. In particular, appendix \ref{appendix:trapezoidal} shows that {\bf the $N$-point trapezoidal rule is exact for bandlimited functions of bandwidth $w$ for all $N > w$}. Hence by using 2D trapezoidal quadrature (Remark \ref{rem:2D_quadrature}, appendix \ref{appendix:trapezoidal}) for $D = \bb T^2$, the expression in (\ref{eq:ano_nonlocality}) becomes
        \begin{equation}\label{eq:ano_newform}
            \frac{1}{|D|} \int_D v(y) dy = \frac{1}{|D|}\left(\frac{1}{N}\right)^2 \sum_{i=1}^{N} \sum_{j=1}^{N} v(z_{ij})
        \end{equation}
        where the period of the function $v$ is $1$ ($\because D =\bb T^2$), $N$ is the number of grid points such that the grid size $\leq 1/2w$ to satisfy the requirements of the Whittaker-Shannon-Kotelnikov sampling theorem \cite{MU2000}. $z_{ij}$ are the grid points.

        \noindent Now, we can modify the equation \ref{eq:convop} by taking $k=N$ as the kernel size and then the equations \ref{eq:convop} and \ref{eq:ano_newform} are same.
        
        \begin{rem}
            Note that in equation \ref{eq:convop} $v$ has been evaluated at a shifted grid $x-z_{ij}$ but still the final answer will be same as the integral until the number of evaluation points are $N$ and the specific discretization points will not matter. This is because exactness of the trapezoidal rule for bandlimited functions depends on $N >w$ and not on specific discretization points (Cororllary 3.3, \cite{LT2014}).
        \end{rem}
        \paragraph{\bf How does $\cal U$ and $\cal D$ operators affects nonlocality?} \label{par:1} The upsampling operator $\cal U$ and downsampling operator $\cal D$ changes the bandwidth $w$ of the input functions. They either increase it (upsampling) or decrease it (downsampling). The exactness of trapezoidal rule as shown in (Corollary 3.3, \cite{LT2014}) depends on $N$, number of grid points. While upsampling if bandwidth has been increased to $\overline{w} > w$, then exactness still remains valid since exactness is true for all $N > w$ and in particular $N > \overline{w} > w$.

        \noindent While downsampling to the bandwidth $\underline{w} < w$, exactness is trivially satisfied since $N > w > \underline{w}$ and therefore there always exists $N$ which guarentees exactness. Now from equation \ref{eq:cno_activation}, we note that $\cal P_l$ is either $\cal D$ or $\cal U$ hence it can be handled as mentioned in the above paragraph.
        $$ v_{l+1} = \cal P_l \circ \cal D_{\overline{w},w} \circ \sigma \circ \cal U_{w,\tilde{w}} \circ \cal K_l(v_l), \qquad 1 \leq l \leq L-1 $$
        In above discussion, we have shown that there always exists $N > w$ that guarentees exactness hence the nonlocality of ANO can be exactly represented in discrete form by $N$- trapezoidal rule. This shows that hidden layer of CNO for specific choice of weights given in \ref{eq:kernel_cno} recovers ANO. Therefore, as an immediate consequence of the universal approximation (Theorem 2.2, \cite{SL2024}) we obtain:
        \begin{cor}[Convolutional Neural Operator]
            The Convolutional neural operator architecture is universal in the setting of Theorem 2.2 of \cite{SL2024}.
        \end{cor}
    \end{proof}















    \appendix
    \section{\bf Detailed proofs}
    \subsection[A.1]{Approximation of Operators mapping between Sobolev spaces by operators mapping between spaces of bandlimited functions} \label{appendix:A.1}
    We prove that one can approximate any continuous operator $\cal G^\dag : \cal X \to \cal Y$ by an operator mapping between spaces of bandlimited functions to arbitrary accuracy. We obtain this result by dsicarding the high-frequency components, higher than the frequency $w$, of both the input and the output of $\cal G^\dag$ by a Fourier projection $P_w$. For orthogonal Fourier projections and also trigonometric polynomial interpolation, the following result on the accuracy of the projection holds,

    \begin{lem*}[A.1] \label{lemA1}
        Given $\sigma, r \in \bb N_0$ with $r > d/2$ and $r \geq \sigma$, and $f \in C^r(\bb T^d)$ it holds for every $w \in \bb N$ that, 
        \begin{equation}
            \|f - P_w(f)\|_{H^\sigma(\bb T^d)} \leq C(r,d)w^{-(r-\sigma)}\|f\|_{H^r(\bb T^d)},
        \end{equation}
        for a constant $C(r,d) > 0$ that only depends on $r$ and $d$.
    \end{lem*}
    By choosing an appropriate frequency cutoff and then discarding the high frequencies of the input and output of $\cal G^\dag$ one can approximate $\cal G^\dag$ to arbitrary accuracy as shown in the result below.
    
    \begin{lem*}[A.2]
        For any $\epsilon, B > 0$ there exist $w \in \bb N$ such that $\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2(D)} \leq \epsilon$ for all $a \in H^r(D)$ with $\|a\|_{H^r(D)} \leq B$.        
    \end{lem*}
    \begin{proof}
        Using Lemma \ref{lemA1} and stability of $\cal G^\dag$, equation \ref{eq2.1}, we have
        $$
        \begin{aligned}
            \|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2} &\leq \|\cal G^\dag(a) - P_w\cal G^\dag(a)\|_{L^2} + \|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\|_{L^2}
        \end{aligned}
        $$
        Since, $\cal G^\dag : H^r(D;\bb R^{d_{\cal X}}) \to H^s(D;\bb R^{d_{\cal Y}})$, we have $\cal G(a) \in H^s(D;\bb R^{d_{\cal Y}})$. In order to apply Lemma \ref{lemA1}, we need to show that $\cal G^\dag(a) \in C^r(D)$. Note that $H^s(D) \hookrightarrow C^r(D)$ for all $s > r + d/2$ which will imply that $\cal G^\dag (a) \in C^r(D)$. Also, note that $P_w$(Fourier projection operator) is {\it non-expansive} ($L^2$-norm is less than $1$). Taking $\sigma = 0$ in Lemma \ref{lemA1}, we have 
        \begin{equation}            
        \begin{aligned}
            \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} &\leq \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{L^2} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \\
            (\because H^0(D) = L^2(D)) \qquad &= \left\|\cal G^\dag(a) - P_w\cal G^\dag(a)\right\|_{H^0} + \left\|P_w\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2}\\
            (\text{Lemma A.1 and non-expansive}) \qquad &\lesssim w^{-r} \left\|\cal G^\dag (a)\right\|_{H^r} + \left\|\cal G^\dag (a) - \cal G^\dag(P_wa)\right\|_{L^2}\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega(\|a - P_wa\|_{H^\sigma})\\
            &\lesssim w^{-r}\|\cal G^\dag\|_{op}\|a\|_{H^r} + \omega (Cw^{-(r-\sigma)}\|a\|_{H^r}).
        \end{aligned}
    \end{equation}
    \end{proof}
    Since above relation is true for all $a$ such that $\|a\|_{H^r} \leq B$, it follows that for large enough $w$:
    \begin{equation}
        \sup\limits_{\|a\|_{H^r}\leq B} \left\|\cal G^\dag(a) - P_w\cal G^\dag(P_wa)\right\|_{L^2} \leq \epsilon.
    \end{equation}
    Given that both $P_wa \in B_w(D)$ and $P_w\cal G^\dag(P_wa) \leq B_w(D)$, a consequence of the above lemma is the existence of an operator $\cal G^* " B_w(D) \to B_w(D):a \mapsto P_w(\cal G^\dag(a))$ that can approximate $\cal G^\dag$ arbitrarily well. Hence $\left\|\cal G^\dag - \cal G^*\right\|_{op} \leq \epsilon$, where the operator are considered as mapping from and to $B_w(D) \cap H^r(D)$ equipped with the $H^r(D)$-norm.

    \subsection{Continuous-Discrete Equivalence for Operator \texorpdfstring{$\cal G^*$}{}} \label{appendix:A.2}
    For every $w > 0$, we denote by $B_w(\bb R^2)$ the space of multivariate bandlimited functions
    $$B_w(\bb R^2) = \{f \in L^2(\bb R^2) : \text{supp}\hat{f} \subseteq [-w,w]^2\},$$
    where $\hat{f}$ denotes the Fourier transform on $L^1(\bb R)$ 
    $$ \hat{f}(\xi) := \int_{\bb R}f(x) e^{-2\pi ix\xi} dx, \qquad \xi \in \bb R,$$
    which extends to $L^2(\bb R)$ by a classical density argument ($L^1(\bb R^n) \cap L^2(\bb R^n)$ is dense in $L^2(\bb R^n), n \geq 1$.) The set $\Psi_w = \{\sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n)\}_{m,n \in \bb Z}$ constitutes an orthonormal basis for $B_w(\bb R^2)$. The bounded operator
    $$ T_{\Psi_w} :l^2(\bb Z^2) \to B_w(\bb R^2), ~~ T_{\Psi_w}(c_{m,n}) = \sum_{m,n\in \bb Z} c_{m,n} \sinc(2w\cdot - m)\cdot \sinc(2w\cdot -n),$$
    which reconstructs a function from its basis coefficients, is called {\it synthesis operator,} and its adjoint 
    $$ T^*_{\Psi_w} : B_w(\bb R^2) \to l^2(\bb Z^2), \qquad T^*_{\Psi_w} f = \left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}_{m,n \in \bb Z},$$
    which extracts basis coefficients from an underlying function, is called {\it analysis operator}.  Every bandlimited function can be uniquely and stably recovered from its sampled values $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$ for ${m,n \in \bb Z}$ via the reconstruction formula
    \begin{equation}
        f(x_1,x_2) = T_{\Psi_w}T^*_{\Psi_w}f(x_1,x_2) = \sum\limits_{m,n \in \bb Z} f\left(\frac{m}{2w},\frac{n}{2w}\right) \sinc(2wx_1 - m) \cdot \sinc(2wx_2 - n),
    \end{equation}
    and we say that there is a {\it continuous-discrete equivalence (CDE)} between $f$ and its samples $\left\{f\left(\frac{m}{2w},\frac{n}{2w}\right)\right\}$. In general, every bandlimited function $f \in B_w(\bb R^2)$ cam be uniquely and stably recovered from its sample values $\{f(mT,nT)\}_{m,n\in\bb Z}$ if the {\it sampling rate} or reciprocal of grid size, $1/T$ is greater or equal than the {\it Nyquist rate} $2w$. This simply follows from the fact that $B_w(\bb R^2) \subset B_{w'}(\bb R^2)$ for every $w' > w$. On the contrary, reconstructing $f \in B_w$ at a sampling rate below the Nyquist rate, i.e. $1/T < 2w$, results in a non-zero value for the {\it aliasing error function:}
    $$ \epsilon(f) = f - T_{{\Psi}_{\frac{1}{2T}}} T^*_{{\Psi}_{\frac{1}{2T}}}f,$$
    and the associated {\it aliasing error} $\|\epsilon\|_2$  (see defintion below).
    \begin{defn}[Aliasing for bandlimited functions]
        The aliasing error function $\varepsilon(f)$ and the corresponding aliasing error of $f \in L^2(\bb R)$ for sampling at the rate $2\Omega$ are given by
        $$ \varepsilon(f) = f - \cal P_{B_\Omega}f, \qquad\qquad \|\varepsilon(f)\|_2 = \|f - \cal P_{B_\Omega}f\|_2.$$
        where $\cal P_{B_\Omega} : L^2(\bb R) \to B_\Omega$ is the {\it orthogonal projection operator} onto $B_\Omega$ given by $P_{B_\Omega}f = \sum_{n\in \bb Z}\langle f,\phi_n\rangle\phi_n$. If the aliasing error $\varepsilon(f)$ is zero, i.e. if $f \in B_\Omega$, we say that there is a continuous-discrete equivalence (CDE) between $f$ and its samples $\{f(n/2\Omega)\}_{n \in \bb Z}$.
    \end{defn}
    \noindent Let $\cal G^*$ be a (possibly) non-linear operator between band-limited spaces, i.e. $\cal G^* :B_w(\bb R^2) \to B_{w'}(\bb R^2)$, for some $w,w' > 0$. As given in \ref{appendix:C}, the concepts of continuous-discrete equivalence (CDE) and aliasing error can be adapted to the operator $\cal G^*$. The continuous operator $\cal G^*$ is uniquely determined by a map $\mathfrak{g}_{\Psi_w, \Psi_{w'}}:\ell^2(\bb Z^2) \to \ell^2(\bb Z^2)$ if the aliasing error operator.
    \begin{equation}\label{eq:opaliaserror}
        \varepsilon = \cal G^* - T_{\Psi_{w'}} \circ \mathfrak{g}_{\Psi_w, \Psi_{w'}} \circ T^*_{\Psi_w}
    \end{equation}
    is identically zero, and we say that $\cal G^*$ and $\mathfrak{g}_{\Psi_w, \Psi_{w'}}$, satisfy a continuous-discrete equivalence (Definition \ref{def:opalias}). Equivalently, the diagram \\
    \begin{figure}[!ht]
        \centering
        \begin{tikzcd}
                B_w \arrow[r, "\cal G^*"] \arrow[d, blue, "T^*_{\Psi_w}"] & B_{w'}, \\
                \ell^2(\bb Z^2) \arrow[r, blue, "\mathfrak{g}_{\Psi_w, \Psi_{w'}}"] & \ell^2(\bb Z^2) \arrow[u, blue, "T_{\Psi_{w'}}"]
        \end{tikzcd}
        \caption{}
    \end{figure}

    \noindent commutes, i.e. the black and the blue directed paths in the diagram lead to the same result. In the latter case, since $T^*_{\Psi_w} \circ T_{\Psi_w}$ is identity operator from $\ell^2(\bb Z^2)$ onto itself ($\because \Psi_w$ is taken to be orthonormal basis set, otherwise it is Gram matrix $G_{mn} := \langle \Psi_n,\Psi_m\rangle$), equation \ref{eq:opaliaserror} forces the discretization $\mathfrak{g}_{\Psi_w,\Psi_{w'}},$ to be defined as
    \begin{equation}\label{eq:defg}
        \mathfrak{g}_{\Psi_w,\Psi_{w'}} := T^*_{\Psi_{w'}} \circ \cal G^* \circ T_{\Psi_w}.
    \end{equation}
    The above defintion is motivated from the fact that $\Psi_w$ is orthonormal set and the equation \ref{eq:opaliaserror} when $\varepsilon = 0$.
    \begin{rem}\label{rem:invert}
        Note that in general $T_{\Psi_w}$ is not invertible. When the frame sequence is orthonormal, it is invertible and its inverse is same as its adjoint.
    \end{rem}
    \begin{rem}
        While isolating $\mathfrak{g}_{\Psi_w,\Psi_{w'}}$ on LHS in equation \ref{eq:opaliaserror}, the term $T^{-1}_{\Psi_{w'}}$ will appear and it has been replaced to $T^*_{\Psi_{w'}}$ in equation \ref{eq:defg} because of remark \ref{rem:invert}. Why? That is how we define it. 
    \end{rem}
    \noindent The definition \ref{eq:defg} is equivalent to say that the diagram,
    \begin{figure}[!ht]
        \begin{tikzcd}
            B_w \arrow[r, blue, "\cal G^*"] & B_{w'} \arrow[d, blue, "T^*_{\Psi_{w'}}"], \\
            \ell^2(\bb Z^2) \arrow[r, "\mathfrak{g}_{\Psi_w, \Psi_{w'}}"] \arrow[u, blue, "T_{\Psi_w}"] & \ell^2(\bb Z^2) 
        \end{tikzcd}
        \caption{}
    \end{figure}
    commutes. In other words, once we fix the discrete representation associcated to the input and output functions, there exists a unique way to defined a discretization $\mathfrak{g}_{\Psi_w,\Psi_{w'}}$ that is consistent with the continuous operator $\cal G^*$ and this is given by \ref{eq:defg}.

    \noindent In practice, we may have access to different discrete representations of the input and output functions, e.g. point samples evaluated on different grids, which in the theory amounts to change of reference systems in the functions spaces. For instance, sampling a function $f\in B_w$ on a finer gris $\left\{(\frac{m}{2\overline{w}},\frac{n}{2\overline{w}})\right\}_{m,n \in \bb Z},\overline{w} > w,$ amounts to representing the function $f$ with respect to the system $\Psi_{\overline{w}} = \{\sinc(2\overline{w}x_1 - m) \cdot \sinc(2\overline{w}x_2 - n)\}_{m,n \in \bb Z},$ which constitutes an orthonormal basis for $B_{\overline{w}} \supset B_w$. Then, one can define the associated CDE discretization $\mathfrak{g}_{\Psi_{\overline{w}},\Psi_{\overline{w}'}}$ as in \ref{eq:defg}, and by equation \ref{eq:opaliaserror}, one readily obtains the change of basis formula
    \begin{equation}
        \mathfrak{g}_{\Psi_{\overline{w}},\Psi_{\overline{w}'}} = T^*_{\Psi_{\overline{w}'}} \circ T_{\Psi_{w'}} \circ \mathfrak{g}_{\Psi_w,\Psi_{w'}} \circ T^*_{\Psi_w} \circ T_{\Psi_{\overline{w}}},
    \end{equation}
    Finally, all the above concepts generalize to every pair of frame sequences $(\Psi,\Psi)$ that span respectively the input and output function spaces, and we refer \cite{FB2023} for a complete exposition.

    \subsection{Multi-channel versions of elementary operators for CNO}\label{appendix:A.3}
    This section contains the {\it multi-channel} versions of the elementary mappings which define CNO \ref{def:CNO}.

    \paragraph{\bf Convolution Operator} In the multi-channel settings, discrete kernels $K_w$ are defined on the $d_{in} \times d_{out} \times s^2$ uniform grids on $D$, where $d_{in}$ is the number of input channels and $d_{out}$ is the number of output channels. Formally, the kernels are defined as
    $$ K_{w,cl} = \sum_{i,j=1}^{k} k_{i,j,cl} \cdot \delta_{z_{ij}}.$$
    where $c$ is the channel index in the input space, while $l$ is the channel index in the output space. Each pair of channels defines corresponding single-channel convolution operation $\cal K_{w,cl} : B_w(D) \to B_w(D)$. For $a \in B_w(D,\bb R^{d_{in}})$, the multi-channel convolution operation $K_w$ is defined as
    $$ (K_wa(x))_l = \sum_{c=1}^{d_{in}} \cal K_{w,cl} a_c(x), \qquad l=1, \dots, d_{out}. $$

    \paragraph{\bf Upsampling and Downsampling Operators} To upsample a signal $a \in B_w(D,\bb R^d)$ with $d$ channels from the bandlimit $w > 0$ to the bandlimit $\overline{w} > w$, one should apply the single-channel upsampling operator $\cal U_{w,\overline{w}}$ to each individual channel of the input signal, independently. Formally, for $a \in B_w(D,\bb R^d)$, the multi channel upsampling $\cal U_{w,\overline{w}} : B_w(d,\bb R^d) \to B_{\overline{w}}(D, \bb R^d)$ is defined as
    $$ (\cal U_{u,\overline{w}}a(x))_c = a_c(x), \qquad \forall x\in D,\quad c=1,\dots, d.$$
    The downsampling operator of a signal $a \in B_w(D, \bb R^d)$ from the bandlimit $w > 0$ to the bandlimit $\underline{w} < w$ is defined in a similar manner (independent applications of the single-channel downsampling operators).

    \paragraph{\bf Activation layer} The multi-channel version of the activation layer, namely $\Sigma_{w,\overline{w}} : B_w(D,\bb R^d) \to B_w(D, \bb R^d)$, is realized by applying the single-channel activation layer to each of the $d$ channels, independently.

    \subsection{\bf Discrete operators for CNO}
    In this section, we define the {\it discrete versions} of the elementary mappings in \ref{def:CNO}. Given a {\it discrete}, multi-channel signal $a_s \in \bb R^{s\times s\times d}$ on $s \times s\times d$ uniform grid, we will use the notation $a_s[i,j,c]$ to refer to the $(i,j)$-th coordinate of the $c$-th channel of the signal, where $i,j = 1, \dots, s$ and $c = 1,\dots, d$.

    \paragraph{\bf Convolution operator} Assume that instead of a continuous, single-channel signal $a\in B_w(D)$, one has an access only to its sampled version $a_s \in \bb R^{s\times s}$ on $s\times s$ uniform grid on $D$. Assume that $a_s$ is to be convolved with a {\it discrete} kernel $K_w \in \bb R^{k \times k}$ with $k = 2 \hat{k}+1$. Let $\hat{a}_s \in \bb R^{(s+2\hat{k}) \times (s+2\hat{k})}$ be an extended version of $a_s$ obtained by circular-padding (domain repeats itself so that kernel can slide through without issues at boundary) to zero-padding (adding $0$'s) of $a_s$. The discrete, single-channel convolution $\cal K_s : \bb R^{s\times s} \to \bb R^{s\times s}$ of the signal $a_s$ and the kernel $K_w$ is given by
    $$ \cal K_s(a_s) = (a_s \star K_w)[i,j] = \sum_{m,n=-\hat{k}}^{\hat{k}} K_w[m,n] \cdot \hat{a}_s [i-m,j-n], \qquad i,j = 1,\dots, s,$$
    where indices of $\hat{a}$ outside the range $1,\dots, s$ corresponds to the padded samples. By performing the convolution this way as described above, we make sure that the input and output signals have the same spatial dimension $s\times s$.

    \noindent Let $a_s \in \bb R^{s\times s\times d_{in}}$ be a discrete, multi-channel and $K_w \in \bb R^{k\times k\times d_{in} \times d_{out}}$ a discrete kernel with $k = 2\hat{k}+1$. The multi-channel convolution of $a_s$ and $K_w$ is defined by
    $$(a_s \star K_w)[i,j,l] = \sum_{m,n=-\hat{k}}^{\hat{k}} \sum_{c=1}^{d_{in}} K_w[m,n,c,l] \cdot \hat{a}_s[i-m, j-n, c], \qquad i,j = 1, \dots, s,$$
    where $l$ corresponds to the index of the output channel and $c$ to the index of the input channel.

    \paragraph{\bf Upsampling and Downsampling Operators} Here, we will define the discrete upsampling and downsampling operators. For $w > 0$, let $h_w$ be the interpolation {\it sinc} filter defined in \ref{eq2.5}. For a discrete, single-channel signal $a_s \in \bb R^{s\times s}$, let $(\tilde{a}_s[n])_{n \in \bb Z}$ be its periodic extension into infinite length given by $\tilde{a}_s[n] = a_s[n \mod s]$ for ${n \in \bb Z}$. The discrete upsampling $\cal U_{s,N} : \bb R^{s\times s} \to \bb R^{Ns \times Ns}$ by an {\it integer factor} $N \in \bb N$ of the signal $a_s \in \bb R^{s\times s}$ is done in {\it two} phases:
    \begin{enumerate}
        \item Firstly, in order to increase the number of samples of the signal $a_s$ from $s^2$ to $(Ns)^2$. One transform the signal $a_s$ into the signal $a_{s,\uparrow Ns}$ obtained by separating each two signal samples of $a_s$ with $N-1$ zero-valued samples. It holds that $a_{s,\uparrow Ns} \in R^{Ns \times Ns}$ and 
        $$ a_{s,\uparrow Ns}[i,j] = \mathds{1}_S (i) \cdot \mathds{1}_S(j) \cdot a_s[i \text{ mod } s, j \text{ mod }s], \quad i,j = 1, \dots Ns,$$
        where $S = \{1, s+1, \dots ,(N-1)s + 1\} $ and $\mathds{1}_S$ is the indicator function. The matrix representation for $N=2$ and $s=2$ will look like this:
        $$\begin{bmatrix}
            \times &0 &\times&0 \\
            0 &0&0 &0\\
            \times &0&\times&0\\
            0 &0&0 &0\\
        \end{bmatrix}$$
        where $\times = a_s[i \text{ mod } s, j \text{ mod } s]$ represent non-zero sampled values. It can be seen that each two signal samples are separated by $0$.
        \item Second step is to convolve the periodic extension of $a_{s,\uparrow Ns}$ with the $h_{s/2}$ interpolation filter to eliminate high frequency components. The upsampled signal is formally obtained by
        $$ \cal U_{s,N}(a_s)[i,j] = \sum_{n,m \in \bb Z} \tilde{a}_{s,\uparrow Ns}[n,m] \cdot h_{s/2} (is - ns, js - ms), \quad i,j = 1, \dots, Ns.$$
    \end{enumerate}
    The discrete downsampling $\cal D_{s,N}: \bb R^{s\times s} \to \bb R^{s/N \times s/N}$ by an {\it integer factor} $N \in \bb N$ of the signal $a_s \in \bb R^{s\times s}$ is also done in {\it two} phases (under the assumption that $s/N \in \bb N$):

    \begin{enumerate}
        \item First step is to convolve the periodic extension of $a_s$ with the $h_{s/(2N)}$ interpolation filter to eliminate high frequency content. Formally, the first step is defined by
        $$ a_{s,s/N}[i,j] = \sum_{n,m \in \bb Z} \tilde{a}_s[n,m] \cdot h_{s/(2N)}(is-ns, js-ms), \quad i,j = 1,\dots, s/N.$$
        \item Second step is to decrease the sampling rate of $a_{s,N/s}$ by keeping every $N$-th sample of the signal. The downsampled signal is formally defined by
        $$ \cal D_{s,N}(a_s)[i,j] = a_{s,s/N}[(i-1)s+1, (j-1)s+1], \quad i,j = 1,\dots, s/N.$$
    \end{enumerate}
    Multi-channel discrete upsampling and downsampling are performed by independent applications of the corresponding single-channel operator.

    \paragraph{\bf Activation layer} Given the definition of the discrete operators, the discrete, single-channel activation layer is defined as
    $$\Sigma_s : \bb R^{s\times s} \to \bb R^{s\times s}, \quad \Sigma_s(a_s) = \cal D_{s,N} \circ \sigma \circ \cal U_{s,N}(a_s),$$
    where $\sigma : \bb R\to \bb R$ is an activation function applied point-wise and $N \in \bb N$ is a fixed constant. The multi-channel activation layer is performed by independent applications of the single-channel activation layer.




    \section{\bf An Introduction to Frame Theory} \label{appendix:B}
    \noindent Let $\cal H$ be a separable Hilbert space with inner product $\langle\cdot,\cdot\rangle$ and norm $\|\cdot\|$.
    \begin{defn}[Frame]
        A countable sequence of vectors $\{f_i\}_{i\in I}$ in $\cal H$ is a {\it frame} for $\cal H$ if there exists constants $A,B > 0$ such that for all $f \in \cal H$
        $$ A\|f\|^2 \leq \sum_{i\in I} |\langle f,f_i\rangle|^2 \leq B\|f\|^2.$$
        We say that $\{f_i\}_{i\in I}$ is a {\it tight frame} if $A = B$ and, in particular, a {\it Parseval fram} if $A = B = 1$. 
    \end{defn}
    \begin{rem}
        The condition in the above definition ensure that $f$ can be stably reconstructed from its inner products with the frame elements. Stable reconstruction here implies the well-posedness and bounded in both direction (no explosion or vanishing of norm).
    \end{rem}
    \begin{rem}
        A frame is complete in $\cal H$, stable spanning set of $\cal H$.
    \end{rem}
    Clearly by the Parseval identity, an orthonormal basis for $\cal H$ is a Parseval frame ($\because \|f\|^2 = \sum_{n=1}^{\infty}|\langle x,x_n\rangle|^2$, by Parseval's identity for a separable Hilbert space, the so called generalization of the Pythagorean theorem). The lower inequality implies that 
    $$ \langle f,f_i\rangle = 0, \forall i \in I \implies f = 0,$$
    which is equivalent to 
    $$ \overline{\text{span}}\{f_i : i \in I\} = \cal H.$$
    On the other hand the upper inequality implies that the operator
    $$ T:l^2(I) \to \cal H, \qquad T(\{c_i\}_{i\in I}) = \sum_{i\in I} c_if_i,$$
    is bounded with $\|T\| \leq \sqrt{B}$ (\cite{Christensen2008-gk} Theorem 3.1.3), and we call $T$ the {\it synthesis operator}. Its adjoint is given by 
    $$ T^* : \cal H \to \cal H, \qquad Sf = TT^*f = \sum_{i\in I} \langle f,f_i\rangle f_i,$$
    which is bounded, invertible, self-adjoint and positive operator (\cite{Christensen2008-gk}, Lemma 5.1.6). We note that the frame operator is invertible since it is bounded, being a composition of two bounded operator, and the frame property implies that $\|\text{Id} - B^{-1}S\| < 1$, where Id denotes the identity operator. Furthermore, the pseudo-inverse of the synthesis operator is given by
    $$ T^\dag : \cal H \to l^2(I), \qquad T^\dag f = (\langle f, S^{-1}f_i\rangle)_{i\in I},$$
    (\cite{Christensen2008-gk}, Theorem 5.3.7) and $\|T^\dag\| \leq 1/\sqrt{A}$(\cite{Christensen2008-gk}, Proposition 5.3.8). The composition $TT^\dag$ gives the identity operator on $\cal H$, and consequently every element in $\cal H$ can be reconstructed via the reconstruction formula
    \begin{equation}\label{formula}
        f = TT^\dag f= \sum_{i\in f}\langle f,S^{-1}f_i\rangle f_i = \sum_{i\in I}\langle f,f_i\rangle S^{-1}f_i,
    \end{equation}
    where the series converge unconditionally. Formula \ref{formula} is known as the {\it frame decomposition thoerem} (\cite{Christensen2008-gk}, Theorem 5.1.7). In particular, if $\{f_i\}_{i\in I}$ is a tight frame, then $S = A\text{Id}$ and formula \ref{formula} simply reads
    $$ f = \frac{1}{A}\sum_{i\in I}\langle f,f_i\rangle f_i.$$
    On the other hand, the composition $T^\dag T$ gives the orthogonal projection of $l^2(I)$ onto Ran($T^\dag$)(\cite{Christensen2008-gk}, Lemma 2.5.2).

    \noindent In what follows, we consider sequences which are not complete in $\cal H$, and consequently are not frames for $\cal H$, but they are frames for their closed linear span.
    \begin{defn}[Frame sequence]
        Let $\{v_i\}_{i\in I}$ be a countable sequence of vectors in $\cal H$. We say that $\{v_i\}_{i\in I}$ is a frame sequence {\it if it is a frame for $\overline{span}\{v_i:i\in I\}$.}
    \end{defn}
    \begin{rem}
        A frame sequence may not span all of $\cal H$, but it is a frame for the subspace of $\cal H$ which it does span.
    \end{rem}
    \noindent A frame sequence $\{v_i\}_{i\in I}$ in $\cal H$ with synthesis operator $T : l^2(I) \to \overline{span}\{v_i : i\in I\}$ is a frame for $\cal H$ if and only if $T^*$ is injective, whilst in general $T^*$ is not surjective and consequently $T$ is not injective. We denote $\cal V = \overline{span}\{v_i : i\in I\}$. Then, the orthogonal projection of $\cal H$ onto $\cal V$ is given by
    $$ \cal P_{\cal V}f = TT^\dag = \sum_{i\in I}\langle f,S^{-1}v_i\rangle v_i,$$
    where $S : \cal V \to \cal V$ denotes the frame operator. Hence, reconstruction formula \ref{formula} holds if and only if $f\in \cal V$.
    \subsection{\texorpdfstring{Condition $u:\text{Ran }T^\dag_\Psi \to \text{Ran }T^\dag_\Phi$}{}} \label{appendix:B.1}

    Once the discretization is choosen in the input and output spaces, which amounts to choosing a frame pair $(\Psi \subseteq \cal H,\Phi \subseteq \cal K)$, we want to define a mapping $u : \ell^2(I) \to \ell^2(K)$ which handles such discrete representations. By frame decomposition theorem \ref{formula}, every function in $\cal H$ is uniquely determined by a sequence in $\text{Ran } T^\dag_\Phi$, and analogously every function in $\cal K$ is uniquely determined by a sequence in $\text{Ran }T^\dag_\Phi$. Hence, it is sufficient to define a mapping from $\text{Ran }T^\dag_\Psi$ into $\text{Ran }T^\dag_\Phi$. We ensure that when two different discretization both yield zero aliasing, the change of frame formula holds, and thus the representation equivalence error is zero.



    \section{\bf Alias-Free Framework for Operator Learning} \label{appendix:C}
    \noindent In this section, we will extend this concept of aliasing of function to operators. Let $U$ be a operator mapping between infinite-dimensional function spaces and $u$ be a discrete representation mapping.
    
    \paragraph{\bf Setting}Let $U: \text{Dom }U \subseteq \cal H \to \cal K$ be an operator between two separable Hilbert spaces, and let $\Psi = \{\psi_i\}_{i\in I}$ and $\Psi = \{\phi_k\}_{k \in K}$ be frame sequences for $\cal H$ and $\cal K$, respectively, with synthesis operators $T_\Psi$ and $T_\Phi$. We denote their closed linear spans by $\cal M_\Psi := \overline{\text{span}}\{\psi_i : i \in I\}$ and $\cal M_\Phi := \overline{\text{span}}\{\phi_i : k \in K\}$. We note that by classical frame theory \cite{Christensen2008-gk}, the pseudo-inverses $T_\Psi^\dag$ and $T_\Phi^\dag$, initially defined on $\cal M^\Psi$ and $\cal M_\Psi$, respectively, can in fact be extended to the entire Hilbert spaces, i.e. $T_\Psi^\dag : \cal H \to l^2(I)$ and $T_\Phi^\dag : \cal K \to l^2(K)$ and this extension is given explicitly via the dual frame analysis operator.
    \subsection{\bf Operator Aliasing and Representation Equivalence}
    Once the discretization is chosen -- which is equivalent to choosing the frame sequences for input and output spaces $(\Psi,\Phi)$ -- connecting the continuous operator $U$ with its discrete counterpart $u$ is the notion of operator aliasing. Given any mapping $u : l^2(I) \to l^2(K)$, we can build the operator $T_\Phi \circ u \circ T_\Psi^\dag : \cal H \to \cal K$, whose definition clearly on the choices of the frame sequences that we make on the continuous level. In other words, any mapping $u$ can be interpreted as a discrete representation of an underlying continuous operator, which in general, may differ from the operator $U$, that is of interest here. Hence, in analogy to definition of aliasing for functions in Hilbert space, we can define the aliasing error of $U$ relative to the discrete representation $u$ as,
    \begin{defn}[Operator aliasing]\label{def:opalias}
        The aliasing error operator $\varepsilon(U,u,\Psi,\Phi):\text{Dom }U \subseteq \cal H \to \cal K$ is given by
        $$ \varepsilon(U,u,\Psi,\Phi) = U - T_\Phi \circ u \circ T_\Psi^\dag,$$
        and the corresponding scalar error is $\|\varepsilon(U, u, \Psi, \Phi)\|$, with $\|\cdot\|$ denoting the operator norm.
    \end{defn}
    \noindent An aliasing error of zero implies that the operator $U$ can be perfectly represented by first discretizing the function with $T_\Psi^\dag$, applying $u$, and then reconstructing with $T_\Phi$, or equivalently, that the diagram in Figure \ref{fig:3A} commutes, i.e. black and blue directed paths in the diagram lead to the same result. If the aliasing error is zero, we say that $(U, u, \Psi, \Phi)$ satisfies a {\it continuous-discrete equivalence (CDE)}, implying that accessing the discrete representation $u$ is exactly same as accessing the underlying continuous operator $U$.
    \begin{figure}[!ht]
        \centering
        \begin{subfigure}{0.22\textwidth}
            \centering
            \begin{tikzcd}
                \cal H \arrow[r, "U"] \arrow[d, blue, "T_\Psi^\dag"] & \cal K \\
                \ell^2(I) \arrow[r, blue, "u"] & \ell^2(K) \arrow[u, blue, "T_\Phi"]
            \end{tikzcd}
            \caption{An alias-free operator's diagram.}
            \label{fig:3A}
        \end{subfigure} \hfill
        \begin{subfigure}{0.28\textwidth}
            \begin{tikzcd}
                \cal H \arrow[r, blue, "U"] & \cal K \arrow[d, blue, "T_\Phi^\dag"]\\
                \ell^2(I) \arrow[u, blue, "T_\Psi"] \arrow[r, "u"] & \ell^2(K) 
            \end{tikzcd}
            \caption{ReNO: alias-free $u$ can be constructed by discr the operator $U$ for different discretization given by $\Psi,\Phi$.}
            \label{fig:3B}
        \end{subfigure}\hfill
        \begin{subfigure}{0.39\textwidth}
            \begin{tikzcd}
                & \ell^2(I) \arrow[r, blue, "{u(\Psi,\Phi)}"] & \ell^2(K) \arrow[rd, "\textcolor{blue}{T_\Phi}", blue] & \\
                \mathcal{H} \arrow[ru, blue, "T_\Psi^\dagger"] \arrow[rrr, "U"] & & & \mathcal{K} \arrow[ld, "T_{\Phi'}^\dagger", blue]  \\
                & \ell^2(I') \arrow[r, "{u(\Psi',\Phi')}"] \arrow[lu, "\textcolor{blue}{T_{\Psi'}}", blue] & \ell^2(K') &
            \end{tikzcd}
            \caption{ReNO: discrete representations $u,u'$ for different discretizations are equivalent}
            \label{fig:3C}
        \end{subfigure}
        \caption{Alias-free framework}
    \end{figure}
    In practice, the discrete representation $u$ of the operator $U$ depends on the choice of the frame sequences. This means that -- as mentioned at the beginning of the section -- there is one map for every input/output frame sequence $\Phi,\Psi$. The consistency between operations $u,u'$ with respect to different frame sequences can be evaluated using the following error.
    \begin{defn}[Representation equivalence error] Suppose that $u,u'$ are discrete maps with associated frame sequences $\Psi,\Phi$ and $\Psi',\Phi'$, respectively. Then, the representation equivalence error is given by the function $\tau(u,u') : \ell^2(I) \to \ell^2(K),$ defined as:
    $$ \tau(u,u') = u - T^\dag_\Phi \circ T_{\Phi'} \circ u' \circ T_{\Psi'}^\dag \circ T_\Psi$$
    and the corresponding scalar error is $\|\tau(u,u')\|$.
    \end{defn}
    \noindent Intuitively, this amount to computing each mapping on their given discretization, and comparing them by expressing $u'$ in the frames associated to $u$
    \begin{rem}\label{change_frame}
        When there is no aliasing then the representation equivalence error is $0$ and the change of frame formula holds (equate $\tau(u,u')$ to $0$ to get the change of frame formula.)
    \end{rem}

    \subsection{Representation equivalent Neural Operator (ReNO)}
    We now introduce the concept of R{\it Representation equivalent Neural Operator (ReNO)}. To this end, for any pair $(\Psi,\Phi)$ of frame sequences for $\cal H$ and $\cal K$, we consider a mapping at the discrete level $u(\Psi,\Phi) : \text{Ran}T^\dag_\Psi \to \text{Ran}T^\dag_\Phi,$ which handles the discrete representations of the functions. This map is discretization dependent and changes with the choices of frame sequences. When this is clear from the context, we will refer to $u(\Psi,\Phi)$ simply as $u$. See \ref{appendix:B.1} for the explanation of why $\text{Ran }T^\dag_\Psi$ or $\text{Ran }T^\dag_\Phi$ is choosen as domain and codomain respectively.

    \begin{defn}[Representation equivalent Neural Operators (ReNO)] \label{def:ReNO}
        We say that $(U,u)$ is a ReNO if for every pair $(\Psi,\Phi)$ of frame sequences that satisfy $\text{Dom }U \subseteq \cal M_\Psi$ and $\text{Ran }U \subseteq \cal M_\Phi$, there is no aliasing, i.e. the aliasing error operator is identical to zero:
        \begin{equation}
            \varepsilon (U,u,\Psi,\Phi) = 0
        \end{equation} 
        We will write this property in short as $\varepsilon(U,u) = 0$.
    \end{defn}
    \noindent In other words, the diagram in \ref{fig:3A} commutes for every every pair of $(\Psi,\Phi)$. In this case, the discrete representations $u(\Psi,\Phi)$ are all equivalent, meaning that they uniquely determine the same underlying operator $U$, whenever a continuous-discrete equivalence property holds at the level of the function spaces. The domain and range conditions in above definition simply imply that the frame can adequately represent input and output functions of $U$.

    \begin{rem} \label{rem:frame_change}
        If aliasing error $\varepsilon(U,u,\Psi,\Phi) = 0$ then the assumption that $u(\Psi,\Phi)$ maps $\text{Ran } T^\dag_\Psi \subseteq \ell^2(I)$ into $\text{Ran } T^\dag_\Phi \subseteq \ell^2(K)$ implies that 
        \begin{equation}
            u(\Psi,\Phi) = T^\dag_{\Phi} \circ U \circ T_\Psi.
        \end{equation}
        We observe that this definition of $u(\Psi,\Phi)$ is such that the diagram in Figure \ref{fig:3B} commutes. In other words, once we fix the discrete representation associated to the input and output functions, there exists a unique way to defined a discretization $u(\Psi,\Phi)$ that is consistent with the continuous operator $U$ and this is given by the above equation. 
        \begin{proof}
            If aliasing error $\varepsilon(U,u,\Psi,\Phi) = 0$, then
            \begin{equation}
                U = T_\Phi \circ u(\Psi,\Phi) \circ T^\dag_\Psi.
            \end{equation}
            By above equation, we obtain
            $$ T^\dag_\Phi \circ U \circ T_\Psi = T^\dag_\Phi \circ T_\Phi \circ u(\Psi,\Phi) \circ T^\dag_\Psi \circ T_\Psi = u(\Psi,\Phi).$$
            where the last equality follows from the fact that $T^\dag_\Psi \circ T_\Psi$ is the orthogonal projection onto $(\text{Ker}(T_\Psi))^{\perp} = \text{Ran}(T^\dag_\Psi)$ and, by assumption, $u(\Psi,\Phi)$ maps $\text{Ran }T^\dag_\Psi$ into $\text{Ran }T^\dag_\Phi$.
        \end{proof}
    \end{rem}

    \noindent In partiuclar, Remark \ref{rem:frame_change} directly implies the formula to go from one discrete representation to another, as
    \begin{equation} \label{eq:frame_change}
        u(\Psi',\Phi') = T^\dag_{\Psi'} \circ T_\Phi \circ u(\Psi,\Phi) \circ T^\dag_\Psi \circ T_{\Psi'}
    \end{equation}
    whenever the pairs of frame sequences $(\Psi,\Phi)$ and $(\Psi',\Psi')$ satisfy the conditions in Definition \ref{def:ReNO}. In other words, the diagram in \ref{fig:3C} commutes.

    \begin{prop}[Equivalence of ReNO discrete representations]
        Let $(U,u)$ be a ReNO. For any two frame sequence pairs $(\Psi,\Phi)$ and $(\Psi',\Phi')$ satisfying conditions in Definition \ref{def:ReNO}, we have that 
        $$ \tau(u,u') = 0,$$
        where, by a slight abuse of notation, $u'$ denotes $u(\Psi',\Phi')$.
    \end{prop}
    \begin{proof}
        Immediately implied by the formula \ref{eq:frame_change}. This proposition establishes the link between aliasing and representation equivalence i.e. if aliasing is $0$ then it will imply the representation equivalence error to be $0$.
    \end{proof}

    \section{\bf Trapezoidal rule is exact for bandlimited functions} \label{appendix:trapezoidal} % add a subsection on tensor product rule.
    \noindent For every $w > 0$, we denote the $B_w(D=\bb T^2, \bb R)$ as the space of bandlimited functions
    $$ B_w(D,\bb R) = \{f \in L^2(D, \bb R) : \text{supp}\hat{f} \subseteq [-w,w]^2\},$$
    where $\hat{f}$ denotes the Fourier transform on $L^1(D, \bb R)$ which can be extended to $L^2(D, \bb R)$ by classical density argument. It can be shown that $\bb T^2 \cong ([0,1]\times[0,1])/\sim$ where $\sim$ is an equivalence relation. Hence we can represent $\bb T^2$ as a unit square with identifying $(x,0) \sim (x,1)$ and $(0,y) \sim (1,y)$ i.e periodic boundary. Since $f\in B_w(D,\bb R) \subset L^2(D,\bb R)$, it can be uniquely represented by a convergent Fourier series. As pointed out in \cite{RB2003}, the expression for $f$ will look like
    \begin{equation}\label{eq:D.1}
        f(x,y) = \sum_{|k|< w} \sum_{|l| < w} c_{k,l}~e^{2\pi i (kx+ly)}, \qquad (x,y) \in [0,1]^2 
    \end{equation}
    where $c_{k,l}= \int_{0}^{1}\int_{0}^{1} f_i(x,y)~e^{-2\pi i (kx+ly)} dx dy$.

    \noindent Now, we can extend the Fourier transform to vector-valued function in a natural way i.e. component-wise Fourier transform \cite{TN2012}. Since the function which we are working with is $v \in B_w(D,\bb R^{d_0})$, we can write this as $v(x,y) = (v_1(x,y),v_2(x,y), \dots v_{d_0}(x,y))$ where $v_i(x,y) : D \to \bb R$ for all $i \in \{1, \dots , d_0\}$. From now on we will consider our analysis for a component $v_i(x,y)$ for arbitrary $i$. 

    \noindent As mentioned in the equation \ref{eq:D.1}, $v_i(x,y)$ can be written as a trigonometric polynomial of degree $w$. Furthermore, corollary 3.3 of \cite{LT2014} shows that $N$-point trapezoidal rule is exact for all $N > w$. This is true for $v_i(x,y)$ for arbitrary $i$ hence each component can be exactly represented by the quadrature. In particular, the nonlocality $\fint v(y)dy$ where $v$ is a $d_0$-valued function can be exactly represented by the quadrature rule applied component wise. This proves the claim.
    \begin{rem}\label{rem:2D_quadrature}
        The trapezoidal rule that has been mentioned above is given by
        $$ \int_{D} v(y) dy = \int_{0}^{1} \int_{0}^{1} v(y) dy =  \left(\frac{1}{N}\right)^2 \sum_{i=1}^{N} \sum_{j=1}^{N} v(z_{ij})$$
    This is 2D version of the trapezoidal rule derived in a natural way from 1D rule. Since the double integral can be written as iterated integration ($\because f$ is integrable and Fubini's thoerem), we use 1D trapezoidal rule twice with suitable values of the parameters. More discussion can be found here \cite{kth2013}.
    \end{rem}

\bibliographystyle{plainnat}
\bibliography{ref_cno}
\end{document}