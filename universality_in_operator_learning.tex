%% Credits of this ams template are with respective people. @Devansh1106 neither own this template nor the credits. 
\documentclass[reqno]{amsart}

\usepackage[numbers]{natbib}
\setlength{\bibsep}{8pt}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lineno}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{esint}
\usepackage{fancyhdr}
\setlength{\parskip}{0.5\baselineskip}%
\setlength{\parindent}{20pt}
\theoremstyle{plain}

\pagestyle{fancy}
\fancyhf{} % Clear default headers/footers
\setlength{\headheight}{10.0pt}
% Even pages: Paper title
\fancyhead[LE]{\footnotesize\textit{Nonlocality and Nonlinearity Implies Universality}}

% Odd pages: Author name
\fancyhead[RO]{\footnotesize\textit{Devansh Tripathi}}

\newtheorem*{thm*}{Theorem}
%% this allows for theorems which are not automatically numbered

\renewcommand{\qedsymbol}{$\blacksquare$}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{rem}{Remark}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cal}[1]{\mathcal{#1}}

\hypersetup{
    colorlinks=false,
    linkcolor=blue,    % Internal links (sections, equations)
    citecolor=red,     % Citation links
    urlcolor=magenta   % URLs (DOIs, websites)
}
%% The above lines are for formatting.  In general, you will not want to change these.

\title{Report on the paper ``Nonlocality and Nonlinearity Implies Universality in Operator Learning''}
\author{Devansh Tripathi \\ ETH Z\lowercase{\"urich}}
\thanks{Seminar für Angewandte Mathematik, HG E 62.2, Rämistrasse 101, 8092 Zürich, Switzerland \\ \href{mailto:devansh.tripathi@sam.math.ethz.ch}{\texttt{devansh.tripathi@sam.math.ethz.ch}}}

\begin{document}


\begin{abstract}
    Neural operator architecture approximate operators between infinite dimensional Banach spaces of function. The paper discuss the basic questions about the requirements for universal approximation of neural operator and provide conditions under which neural operators are universal approximators. Author argued that the general approximation of operators between spaces of functions must be both {\it nonlocal} and {\it nonlinear}.

    A popular variant of neural operators is the Fourier neural operator (FNO). Proving universal approximation theorem for FNOs is based on using unbounded number of Fourier modes, this work challenges this point of view and provide a novel minimal architecture called ``averaging neural operator'' (ANO) and its analysis showed that ANO is also a universal approximator. Only spatial average is taken as nonlocal ingredient which corresponds to retaning only a single Fourier mode in the case of FNO contrasts to unbounded number of modes.    
\end{abstract}
\maketitle
\section{\bf \large Introduction}
\noindent The task that we are trying to achieve with the help of neural networks is to approximate the underlying operator, which defines a mapping between two infinite-dimensional Banach spaces of functions. Neural operator generalizs the underlying framework of neural network to infinite-dimensional setting and learn such operators from the data. The wide range of neural operators introduced in \cite{AAK2020, NK2023} are defined in analogy with neural networks, but appends weight matrices in the hidden layers with additional linear integral operators acting on the input functions. Special cases of this framework includes Fourier neural network (FNO) \cite{ZL2021} in which the reliance on a Fourier basis limits the basic form of the FNO to periodic geometries, although, in that setting, use of fast Fourier transform (FFT) allows for efficient computations with total number of Fourier components limited by the grid resolution. Extension of FNO, called Neural Operator on Riemannian Manifold (NORM) \cite{GC2023}, generalizes FNO to use arbitrary orthogonal eigenfunctions of the Laplace-Beltrami operator on any given  spatial domain. The approach mentioned in the paper is closely related to low rank neural operator \cite{NK2023} which, however, has a more complicated architecture that proposed in this paper.

\noindent In the paper author argued that universal approximation can be obtained in general geometries, with nonlocality along with nonlinearity. Nonlocality has been introduced using only a low-rank operator of fixed finite rank, and is not restricted to periodic domains.

\noindent Fourier neural operators are already nonlocal and they introduce it via the addition of a nonlocal operator in each hidden layer layer, which acts on the Fourier modes of the input function by matrix multiplication. FNOs are generally implemented with a first layer which lifts the input, a scalar or vector-valued function, to a vector-valued function where the vector dimension (also called model width) is much higher than that of input function itself. It has been showed in literature that increasing the number of channels (model width) rather than to retain more Fourier modes in the architecture is more beneficial in certain circumstances \cite{SL2022}.

\noindent The author has proposed a underlying framework to many neural operators called averaging neural operator (ANO). The ANO is build upon two minimal ingredients, nonlinearity by composition of shallow neural networks, and nonlocality via a spatial average. Author has deduced many universal approximation theorems for the ANO.

\subsection{\bf Neural Operator}
Let $\Omega \subset \bb R^d$ denote a bounded domain (or potentially a manifold) and let $\cal X(\Omega;\bb R^o)$,$\cal Y(\Omega;\bb R^o)$ and $\cal V(\Omega;\bb R^o)$ denote Banach spaces of $\bb R^o-$ valued functions over $\Omega$. The {\it nonlocal neural operator} (NNO) is defined as a mapping 
$$ \Psi \colon \cal X(\Omega;\bb R^k) \to \cal Y(\Omega;\bb R^k)$$ 
which can be written as composition of the form $\Psi = \cal Q \circ \cal L_L \circ \dots \cal L_1 \circ \cal R$ where $\cal R$ is lifting layer, $\cal L_l, l = 1, \dots L$ are hidden layers and $\cal Q$ is projection layer. Given a channel dimension $d_c$, the {\bf lifting layer} $\cal R$ and {\bf projection layer} $\cal Q$ are given by a mapping respectively:
\begin{align}
    \cal R \colon \cal X(\Omega;\bb R^k) & \to \cal V(\Omega;\bb R^{d_c}),~~~ u(x) \mapsto R(u(x),x), \\
    \cal Q : \cal V(\Omega;\bb R^{d_c}) & \to \cal Y(\Omega;\bb R^{d_c}),~~~v(x) \mapsto Q(v(x),x)
\end{align}
where $R \colon \bb R^k \times \Omega \to \bb R^{d_c}$ and $Q : \bb R^{d_c} \times \Omega \to \bb R^{k'}$ are learnable neural network acting between finite dimensional Euclidean spaces. For $l = 1, \dots, L$ (the number of  {\bf hidden layers}) and for $m = 0, \dots, M$ (the number of modes) choose functions $\psi_{l,m},\phi_{l,m} \colon \Omega \to \bb R^{d_c}$. For $l = 1,\dots, L$, each hidden layer $\cal L_l$ is the mapping $\cal V(\Omega;\bb R^{d_c}) \to \cal V(\Omega;\bb R^{d_c})$ of the form:
$$ (\cal L_lv)(x) := \sigma\left(W_lv(x) + b_l + \sum_{m=0}^M \langle T_{l,m}v,\psi_{l,m}\rangle_{L^2(\Omega;\bb R^{d_c})}\phi_{l,m}(x)\right) $$
where
\begin{enumerate}
    \item $W_l,T_{l,m}\in \bb R^{d_c \times d_c}$ and bias $b_l \in \bb R^{d_c}$ are the learnable parameters/matices.
    \item $\langle T_{l,m}v,\psi_{l,m}\rangle_{L^2(\Omega;\bb R^{d_c})}$ is a inner product in $L^2-$ function space. It measure the contribution of $\psi_{l,m}$ in $T_{l,m}$.
\end{enumerate}
\paragraph{\bf Note}
\begin{enumerate}
    \item The $L^2$ space is space of square integrable functions such that $L^2(\Omega;\bb R^{d_c}) := \left\{f\mid \int_{\Omega} \|f(x)\|^2 dx < \infty \right\}$.
    \item The form of lifting and projection layer allows for {\it positional encoding}.
    \item This general framework reduces to the FNO in a periodic geometry and if the expansion function are choosen as Fourier basis functions, indexed by $m$ and independent of $l$.
\end{enumerate} 
The central questions that author posses: ``which minimal assumptions have to be imposed on the expansion functions $\psi_{l,m}$ and $\phi_{l,m}$, to ensure universal approximation of the resulting architecture?''
\paragraph{\bf Assumptions on activation function:}$\sigma :\bb R \to \bb R$ is assumed to be smooth, $\sigma \in C^{\infty}(\bb R)$, nonpolynomial and Lipschitz continuous. It acts as a Nemitskii-operator, component-wise on inputs.

\section{\bf \large Averaging suffices for Universal Approximation}
\noindent Author proposes a new architecture called averaging neural operator (ANO) and shows that only a simple averaging as nonlocality suffices for universality. The ANO is the subclass of many instantiations of general NNO architecture, and hence implies universality for general NNOs as corollaries.  
\noindent When the domain is periodic, the resulting ANO becomes a special case of FNO when only zeroth Fourier mode is retained.

\subsection{\bf Nonlinearity and Nonlocality}
In case of ordinary neural networks, nonlinearity alone suffies for universality but in case of neural operators, it does not alone suffies. To support this, we have a following example: a neural network with a single layer give rise to nonlinear operator, which maps an input function to an output function by composition, 
$$ u(x) \mapsto \sigma(Wu(x) + b).$$
Despite being nonlinear, it can be shown that they are not universal. For example, such mappings are not able to approximate even simple operators $\Psi^\dag$ with a {\it nonlocal} dependence on the input, such as the shift operator $\Psi^\dag(u)(x) := u(x+h)$ for fixed $h \neq 0$. This shows the need of nonlocality.

\subsection{\bf Averaging Neural Operator: a Special Subclass of the NNO}
The author has defined a special subclass of the NNO, which combines nonlinearity by composition with nonlocality by averaging. We define a special subclass of hidden layers of the form:
\begin{equation}\label{1}
    \cal L : \cal V(\Omega;\bb R^{d_c}) \to \cal V(\Omega;\bb R^{d_c}),~~~ \cal L(v)(x) := \sigma\left(Wv(x) + b+ \fint_\Omega v(y) dy \right)
\end{equation}

\noindent The author has taken up the case of {\it single hidden layer} hence the following architecture:
$$ \Psi:\cal V(\Omega;\bb R^{d_c}) \to \cal V(\Omega;\bb R^{d_c}),~~~ \Psi(u) = \cal Q \circ \cal L \circ \cal R(u),$$
where lifting and projection maps $\cal R$ and $\cal Q$ are given as in above equations with $R$ and $Q$ be single-hidden layer neural networks respectively of width $d_c$. 
\paragraph{\bf Parameters of ANO} Due to its minimal structure, the ANO depends on only one {\it hyperparameter;} the lifting dimension $d_c$. The {\it tunable parameters} of ANO are represented by the weight matrix $W \in \bb R^{d_c\times d_c}$ and bias $b \in \bb R^{d_c}$ in the hidden layer $\cal L$, and the internal weights and biases of the ordinary neural network $R$ and $Q$ defined in lifting and projection layers, respectively.

\subsection{\bf Universal Approximation}
\begin{thm}
    Let $\Omega \subset \bb R^d$ be a bounded domain with Lipschitz boundary. For given integers $s,s' \geq 0$ let $\Psi : C^s(\overline{\Omega};\bb R^k) \to C^{s'}(\overline{\Omega};\bb R^{k'})$ be continuous operator, and fix a compact set $K \subset C^s(\overline{\Omega};\bb R^k)$. Then for any $\epsilon > 0$, there exists an averaging neural operator $\Psi : K \subset C^s(\overline{\Omega};\bb R^k) \to C^{s'}(\overline{\Omega};\bb R^{k'})$ such that 
    $$ \sup\limits_{u \in K}\|\Psi^\dag (u) - \Psi(u)\|_{C^{s'}} \leq \epsilon.$$ 
\end{thm}
The above result is in the space of continuously differentiable functions. Author also has a corresponding result in the scale of Sobolov spaces $W^{s,p}$:
\begin{thm}
    Let $\Omega \subset \bb R^d$ be a bounded domain with Lipschitz boundary. For given integers $s,s'\geq 0$, and reals $p,p' \in [1,\infty)$, let $\Psi^\dag: W^{s,p}(\Omega;\bb R^k) \to W^{s',p'}(\Omega;\bb R^{k'})$ be a continuous operator. Fix a compact set $K \subset W^{s,p}(\Omega;\bb R^k)$ of bounded functions, $\sup_{u \in K}\|u\|_{L^\infty} < \infty$. Then for any $\epsilon > 0$, there exists an averaging neural operator $\Psi:W^{s,p}(\Omega;\bb R^k) \to W^{s',p'}(\Omega;\bb R^{k'})$ such that 
    $$ \sup\limits_{u\in K}\|\Psi^\dag(u) - \Psi(u)\|_{W^{s',p'}} \leq \epsilon.$$
\end{thm}
The consequences of the results is that any more general NNP architecture hidden layers of the form (\ref{1}), is universal as long as an average can be represented; then the resulting NNO reduces to ANO with a specific setting of tunable weights (by setting certain parameters to zero).
\begin{rem}
    Above two theorem show that ANO is universal i approximating a large class of operators, uniformly over a compact set of input functions $K$. But in practice, neural operators are trained by minimizing an empirical loss:
    $$ \cal L(\Psi) = \frac{1}{N} \sum\limits_{n=1}^{N} \|\Psi^\dag(u_n) - \Psi(u_n)\|^2_{W^{s',p'}(\Omega)},~~~~u_1, \dots u_N \sim \mu,$$
    where data are iid random samples form an underlying input probability measure $\mu$. A popular choice is sampling input function from a Gaussian random field. In this case, the set of input functions is no longer bounded, and hence not compact.

    \noindent In this case when $\mu$ does not have a compact support, we can derive these results with a cut-off argument. For given $\epsilon > 0$, it is possible to show that the existence of $\Psi$, such that 
    $$ \bb E_{u\sim\mu}[\|\Psi(u) - \Psi^\dag(u)\|^2_{W^{s',p'}}] < \epsilon.$$
\end{rem}
\subsection{Intuition}
\paragraph{\bf Encoder-Decoder Structure} The ANO architecture has hidden encoder-decoder structure. This structure can be obtained when setting the matrix $W$ and bias $b$ in the hidden layer (\ref{1}) zero, in which case we note that the mapping $\cal L \circ \cal R : \cal X \to \bb R^{d_c}, u \mapsto \cal L \circ \cal R(u)$ can be though of as a nonlinear encoding of the input function and encode it to a (constant) vector $v \in \bb R^{d_c}$ while the projection mapping that takes $v \mapsto Q(v,.)$ is a nonlinear decoding of the corresponding output function $\Psi^\dag(u) = Q(u,x)$.

\paragraph{\bf The Role of Positional Encoding}
Author has argued that some positional encoding (``explicit $x$-dependence'') is necessary for universality. Positional encoding helps in breaking translational equivariance i.e. FNO $\Psi$ does not commute with the shift operator. Breaking translational equivariance is necessary because the PDEs with non-constant coefficients have solution operator which are non-translational equivariant. There is one more way to break translational equivariance that is to have bias function dependent on $x, b = b(x)$. It was used in previous work by the author \cite{NK2021} but in practice positional encoding is explicitly added in the input layer $\cal R$.

\noindent Explicit $x$-dependence in the output layer is not necessary since there are pointwise matrix multiplications involved in the hidden layers and they provide a mechanism (same as ``skip connections'') to forward positional encoding information to the output layer.

\begin{rem}[Skip Connections]
    Skipping connections basically means bypass one or more layer of the neural network. It adds the output of layer($l$) to the output of layer($l+n$):
    $$ y = F(x) + x$$
    where:
    \begin{itemize}
        \item $x$ is the input.
        \item $F(x)$ is the transformation applied to skip layer(s).
        \item $y$ is the output after the skip.
    \end{itemize}
\end{rem}
\subsection{Sketch of the Proof of Universality}
For the sake of completeness, theorem statement has been stated again below:
\begin{thm}
    Let $\Omega \subset \bb R^d$ be a bounded domain with Lipschitz boundary. For given integers $s,s' \geq 0$ let $\Psi : C^s(\overline{\Omega};\bb R^k) \to C^{s'}(\overline{\Omega};\bb R^{k'})$ be continuous operator, and fix a compact set $K \subset C^s(\overline{\Omega};\bb R^k)$. Then for any $\epsilon > 0$, there exists an averaging neural operator $\Psi : K \subset C^s(\overline{\Omega};\bb R^k) \to C^{s'}(\overline{\Omega};\bb R^{k'})$ such that 
    $$ \sup\limits_{u \in K}\|\Psi^\dag (u) - \Psi(u)\|_{C^{s'}} \leq \epsilon.$$ 
\end{thm}
\paragraph{\bf Explanation and definition of terms:}
\paragraph{\bf Lipschitz Domain}
$\Omega \subset \bb R^d$ is a bounded Lipschitz domain and $\overline{\Omega}$ is compact (Heine-Borel theorem). The boundary $\delta\Omega = \overline{\Omega}\backslash\Omega$ is at least ``Lipschitz regular''(locally it is the graph of a Lipschitz function). In particular, any bounded domain with piecewise smooth boundary is a Lipschitz domain.

\paragraph{\bf Function Extension for Lipschitz Domains}
We will be using this extension property in the proof. Given a function $u : \Omega \to \bb R$, defined on a domain $\Omega \subset \bb R^d$, to a function $u :\bb R^d \to \bb R$, defined on all of $\bb R^d$. The following lemma shows that this is possible, while preserving smoothness of $u$.
\begin{lem}[A.1][Periodic extension operator]
    Let $\Omega \subset \bb R^d$ be a bounded Lipschitz domain. There exists a continuous, linear operator $\cal E : W^{s,p}(\Omega) \to W_{per}^(s,p)(B)$ for any $s \geq 0$ and $p \in [1,\infty],$ where $B \subset \bb R^d$ is a bounded hypercube containing $\Omega \subset B$, such that for any $u \in W^{s,p}(\Omega)$:
    \begin{enumerate}
        \item $\cal E(u)\mid_\Omega = u;$
        \item $\cal E(u) \in W^{s,p}_{per}(B)$ is periodic on $B$ (including it's derivatives).
    \end{enumerate}
    Furthermore, $\cal E$ maps continuously differentiable functions to continuously differentiable functions, i.e. $\cal E(C^s(\overline{\Omega})) \subset C^s_{per}(B)$ and hence defines a continuous mapping $\cal E: C^s(\overline{\Omega}) \to C^s_{per}(B)$.
\end{lem}
\begin{rem}
    An operator $A$ is said to be continuous if it preserves the limit i.e. if a sequence $\{x_n\}$ converges to the limit $x$ then we have $Ax_n \to Ax$. For linear operator between normed spaces, continuity is equivalent to boundedness.
\end{rem}
\paragraph{\bf Mollification (Smoothing) of Functions om Lipschitz Domains}
There exists a smooth mapping (a mollifier) $\rho : \bb R^d \to \bb R$, with the properties $p(x) \in [0,1]$ for all $x \in \bb R^d$.
$$
\rho(x) = \begin{cases}
    =1, &\text{if $x=0$}\\
    =0, &\text{if $|x| \geq 1$}\\
    \in [0,1], &\text{if $0 < x < 1$}
\end{cases}
$$
Normalization of $\rho$ can be done so that $\int_{\bb R^d} \rho(y)dy = 1$. With any such $\rho$, we can define a family of functions $\rho_{\delta} := \delta^{-d}\rho(x/\delta)$, with support in a $\delta-$ball around origin. Fixing such a family, $\epsilon$-mollification of a function $u : \bb R^d \to \bb R, u \in L^1(\bb R^d)$ is defined by a convolution $u_{\delta}(x):= (u*\rho_\delta)(x),$ i.e. 
$$ u_\delta(x) := \int_{\bb R^d} u(x-y)\rho_\delta(y)dy.$$
$u_\delta$ is smooth function for $\delta>0$ (by lemma \ref{2}) and that 
\begin{lem}\label{2}
    If $g \in C^\infty_c(\bb R^n)$ and %understand R proof and then extend it to R^n and then write that here.
\end{lem}
\begin{lem}[A.2](Adapted mollification in a Lipschitz domain) \label{A.2}
    Let $\Omega \subset \bb R^d$ be a bounded Lipschitz domain. There exists a one-parameter family $\cal M_\delta$ of ``mollification'' operators, indexed by $\delta \geq 0$ and defining a linear mapping $\cal M_\delta :L^1(\Omega;\bb R^k) \to L^1(\Omega; \bb R^k)$ for $\delta > 0$, such that:
    \begin{enumerate}
        \item For $\delta = 0$, we have $\cal M_0u = u$ for all $u \in L^1(\Omega \bb R^k)$; for $\delta > 0$, $\cal M_\delta$ is smoothing, in the sense that it defines a mapping $\cal M_\delta :L^1(\Omega;\bb R^k)\to C^\infty(\overline{\Omega};\bb R^k)$.
        \item For any fixed $\delta > 0$, integer $s,r \geq 0$, the mapping, $$ \cal M_\delta : C^s(\overline{\Omega};\bb R^k) \to C^r(\overline{\Omega};\bb R^k),$$ is continuous. Furthermore, if $r \leq s$, then the operator norm is uniformly bounded, that is $$\sup\limits_{\delta>0} \|\cal M_\delta\|_{C^s \to C^r} < \infty.$$
        \item If $s \geq 0$ and $K \subset C^s(\overline{\Omega};\bb R^k)$ is a compact subset, then for any $\delta_0 \geq 0$, $$ \lim_{\delta \to \delta_0} \sup_{u \in K} \|\cal M_{\delta_0} u - \cal M_\delta u\|_{C^s} = 0$$.
        \item For fixed $\delta > 0$, integer $s,r \geq 0$, and $p \in [1,\infty)$, the mapping $$\cal M_\delta : W^{s,p}(\Omega;\bb R^k) \to W^{r,p}(\Omega;\bb R^k)$$ is continuous.
        \item If $s \geq 0, p \in [1, \infty)$ and $K \subset W^{s,p}(\Omega;\bb R^k)$ is a compact subset, and $\delta_0 \geq 0$, then $$ \lim_{\delta \to \delta_0}\sup_{u \in K}\|\cal M_{\delta_0}u - \cal M_\delta u\|_{W^{s,p}} = 0.$$
    \end{enumerate}
\end{lem}
\begin{lem}[A.4]
    Fix $s \geq 0$. Let $K \subset C^s(\overline{\Omega};\bb R^k)$ be compact. Then for any $\delta > 0$, the set 
    $$ K_\delta := \bigcup\limits_{0\leq \delta' \leq \delta} \{\cal M_{\delta'}u \mid u\in K\},$$
    is also compact in $C^s(\overline{\Omega};\bb R^k).$
\end{lem}
\begin{proof}
    Since $C^s(\overline{\Omega};\mathbb{R}^k)$ is a normed space with respect to $C^s$ norm, it will be metric space with respect to induced metric from norm. Hence, it is enough to show that subset $K_\delta$ is sequentially compact.

    Let $v_1, v_2, \dots$ be an arbitrary sequence in $K_\delta$. It suffices to show that $v_j$ posses a convergent subsequence $v_{j_l} \to v \in K_\delta$. By definition, there exist a sequence $u_j \in K$ and $\delta_j \in (0,\delta]$ such that $\cal M_{\delta_j}u_j = v_j$ for all $j \in \bb N$. Since, $K$ is compact, there exists a convergent subsequence $u_{j_l} \to u \in K$. Furthermore, we may assume that $\delta_{l_j} \to \delta_\infty \in [0,\delta]$ converges to a limit. (but we need to show its convergence. TODO.) Let $v := M_{\delta_\infty}u$ and $v \in K_\delta$. We claim that $v_{j_l} \to v$. 
    $$ \begin{aligned}
        \limsup\limits_{l \to \infty} \|v_{j_l} - v\|_{C^s} &= \limsup\limits_{l \to \infty} \|\cal M_{\delta_{j_l}}u_{j_l} - \cal M_{\delta_{\infty}}u\|_{C^s}\\
        (\text{$u \in C^s$ is fixed}) &=  \limsup\limits_{l \to \infty} \|\cal M_{\delta_{j_l}}u_{j_l} - \cal M_{\delta_{j_l}}u + \cal M_{\delta_{j_l}}u - \cal M_{\delta_{\infty}}u\|_{C^s} \\
        &\leq \limsup\limits_{l \to \infty} \|\cal M_{\delta_{j_l}}u_{j_l} - \cal M_{\delta_{j_l}}u\|_{C^s} + \limsup\limits_{l \to \infty} \|\cal M_{\delta_{j_l}}u - \cal M_{\delta_{\infty}}u\|_{C^s} \\
        &= \limsup\limits_{l \to \infty} \|\cal M_{\delta_{j_l}}\|_{C^s \to C^s} \|u_{j_l} - u\|_{C^s}\\ &\qquad\qquad+ \limsup\limits_{l \to \infty} \|\cal M_{\delta_{j_l}}u - \cal M_{\delta_{\infty}}u\|_{C^s}
    \end{aligned}
    $$
    From Lemma \ref{A.2} (part $3$), we can say second part converges to $0$. Again from Lemma \ref{A.2} (part $2$), $\cal M_\delta$ is uniformly bounded 
    $$ \sup_{l\in \bb N}\|\cal M_{\delta_{j_l}}\|_{C^s \to C^s} \leq \sup_{0 \leq \delta' \leq \delta}\|\cal M_{\delta'}\|_{C^s \to C^s} < \infty $$ (why so? since $\delta_j \in (0,\delta])$; $\delta_{j_l}$ is the subsequence of $\delta_j$ and if the subsequence has the supremum of whole interval $[0,\delta]$ then its equality otherwise it has to be less than because supremum is taken in whole $[0,\delta]$. Also, since $K$ is compact, $u_{j_l} \to u$ which implies first part also tends to $0$. Hence, we have $v_{j_l} \to v$ and $K_\delta$ is sequentially compact hence compact. 
\end{proof}
\paragraph{\bf Universal Approximation Of Neural Networks}
From [\cite{AP1999}, Thm. 4.1], neural network architecture is universal in the class of $C^s$- function between Euclidean vector spaces ($C^s(\bb R^n; \bb R^m)$) when the activation function $\sigma$ is nonpolynomial and sufficiently smooth, $\sigma \in C^s$. This implies universality of neural networks in Sobolev spaces $W^{s,p}$ of functions between Euclidean vector spaces. (Why? TODO).
\begin{lem}[A.5] TODO
\end{lem}
\begin{proof}
    TODO
\end{proof}
\paragraph{\bf A Dense Subset Of Operators}
In order to prove universal approximation for averaging neural operator is the fact that it is possible to reduce the problem for general operator $\Psi^\dag :C^s(\overline{\Omega};\bb R^k) \to C^{s'}(\overline{\Omega};\bb R^{k'})($or $\Psi^\dag :W^{s,p}(\overline{\Omega};\bb R^k) \to W^{s',p'}(\overline{\Omega};\bb R^{k'}$, respectively), to a simple class of operators which can be written in the form,
$$ \tilde{\Psi}^\dag(u) = \sum\limits_{j=1}^{J}\alpha_j(u)\eta_j,$$
where $\eta_1,\dots,\eta_j$ are functions in $C^{s'}(\overline{\Omega};\bb R^{k'})$(resp. in $W^{s',p'}(\overline{\Omega};\bb R^{k'})$), and $\alpha_1,\dots,\alpha_J:L^1(\Omega;\bb R^k) \to \bb R$ are continuous nonlinear functionals, defined on the space of integrable functions.
\bibliographystyle{plainnat}
\bibliography{ref}
\end{document}